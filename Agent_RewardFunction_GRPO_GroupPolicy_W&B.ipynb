{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Reinforcement-Learning/blob/main/Agent_RewardFunction_GRPO_GroupPolicy_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage (assuming 'contexts' and 'ground_truth' are defined):\n",
        "# combined_context = \" \".join(contexts[0]) # Combine retrieved contexts\n",
        "# reward = cosine_similarity_reward(combined_context, ground_truth[0])\n",
        "# print(f\"Cosine Similarity Reward: {reward}\")"
      ],
      "metadata": {
        "id": "wtwCsHlESzC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers  = []\n",
        "contexts = []\n",
        "cosine_similarity_rewards = [] # List to store cosine similarity rewards\n",
        "\n",
        "\n",
        "# traversing each question and passing into the chain to get answer from the system\n",
        "# Define the retriever from the OECD index\n",
        "retriever = CombinedFinancial10k_index.as_retriever()\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "    response = agent.chat(question)\n",
        "    answers.append(response.response) # Extract the string response\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_texts = [docs.node.text for docs in retrieved_docs]\n",
        "    contexts.append(context_texts)\n",
        "\n",
        "    # Calculate cosine similarity reward\n",
        "    # Combine retrieved contexts into a single string for similarity calculation\n",
        "    combined_context = \" \".join(context_texts)\n",
        "    cosine_similarity_reward_score = cosine_similarity_reward(combined_context, ground_truth[i])\n",
        "    cosine_similarity_rewards.append(cosine_similarity_reward_score)\n",
        "\n",
        "\n",
        "# Preparing the dataset\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"ground_truth\": ground_truth,\n",
        "    \"contexts\": contexts, # Add the contexts to the dataset\n",
        "    \"cosine_similarity_reward\": cosine_similarity_rewards, # Add the cosine similarity rewards\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset.to_pandas()"
      ],
      "metadata": {
        "id": "IMZHDx0B_dhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afbd2a"
      },
      "source": [
        "# Task\n",
        "Calculate rewards based on cosine similarity, explain how to update the policy directly in GRPO for the current situation, and define the policy network for this example, specifically referring to Group Relative Policy Optimization (GRPO) in the context of a RAG system where the reward is based on cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5a3b13"
      },
      "source": [
        "## Understand the role of the policy network\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of the RAG system the policy network will control (e.g., retrieval parameters, generation parameters, or both).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0389c9"
      },
      "source": [
        "## Choose a suitable neural network architecture\n",
        "\n",
        "### Subtask:\n",
        "Select a type of neural network (e.g., a feedforward network, recurrent network, or transformer-based model) that can process the input question and output the control signals for the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c336904d"
      },
      "source": [
        "**Reasoning**:\n",
        "To select a neural network architecture suitable for processing text input (the question) and outputting control signals for the RAG system, considering the sequential nature of text and the need to capture semantic meaning, a transformer-based model is a good choice due to its effectiveness in handling sequential data and complex relationships within text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b210d0"
      },
      "source": [
        "# Explanation of the chosen neural network architecture\n",
        "\n",
        "# For this task, where the input is a text question and the output needs to be control signals\n",
        "# for a RAG system (e.g., parameters for retrieval or generation), a transformer-based model\n",
        "# is a suitable choice.\n",
        "\n",
        "# Justification:\n",
        "# 1. Text Input: Transformers are specifically designed to process sequential data like text.\n",
        "# 2. Semantic Understanding: They excel at capturing long-range dependencies and semantic relationships\n",
        "#    within the input question, which is crucial for determining appropriate RAG control signals.\n",
        "# 3. Contextual Embeddings: Transformer models generate rich contextual embeddings of the input text,\n",
        "#    allowing the policy network to make informed decisions based on the nuances of the question.\n",
        "# 4. Adaptability: Transformer architectures can be adapted with output layers to produce numerical\n",
        "#    or categorical control signals as needed by the RAG system.\n",
        "\n",
        "# Examples of transformer-based models that could be adapted for this purpose include\n",
        "# models based on BERT, RoBERTa, or smaller, task-specific transformer variants.\n",
        "# The specific output layer would depend on the exact control signals being predicted (e.g.,\n",
        "# a dense layer for numerical parameters or a softmax layer for categorical choices).\n",
        "\n",
        "# This choice aligns with the task of learning a policy based on textual input to influence\n",
        "# the RAG system's behavior for better performance according to the defined reward."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2e884"
      },
      "source": [
        "## Define the input and output layers\n",
        "\n",
        "### Subtask:\n",
        "Define the format of the input (the question) and the output (the parameters or actions that influence the RAG system) for the chosen transformer-based policy network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c5cf8a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the input and output format of the chosen transformer-based policy network based on the previous subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a702d66a"
      },
      "source": [
        "# 1. Input Layer Format:\n",
        "# The input to the policy network will be the user's question, which is a string of text.\n",
        "# Before being fed into the transformer layers, this text will undergo standard NLP preprocessing steps:\n",
        "# - Tokenization: The text will be broken down into a sequence of tokens (words or sub-word units) using a tokenizer appropriate for the chosen transformer model (e.g., WordPiece for BERT, BPE for RoBERTa).\n",
        "# - Embedding: The sequence of tokens will be converted into a sequence of numerical embeddings. Transformer models typically use learned token embeddings, positional embeddings (to capture token order), and potentially segment embeddings. The input to the transformer layers will be a tensor of shape (batch_size, sequence_length, embedding_dim), where:\n",
        "#   - batch_size: The number of questions processed in parallel.\n",
        "#   - sequence_length: The maximum number of tokens in a question (padded or truncated).\n",
        "#   - embedding_dim: The dimensionality of the token embeddings.\n",
        "\n",
        "# 2. Output Layer(s) Format:\n",
        "# The output layer(s) of the policy network will produce control signals for the RAG system. Based on the understanding that the policy network controls retrieval parameters (like similarity_top_k) and potentially influences generation, the output could be structured as follows:\n",
        "# - For a numerical parameter like `similarity_top_k`: A single dense layer with one output neuron, potentially followed by an activation function (e.g., ReLU to ensure non-negativity) and possibly scaled to a reasonable range. The output would be a tensor of shape (batch_size, 1).\n",
        "# - For influencing generation (less direct control in this setup, but conceptually): This could be represented as a vector influencing attention mechanisms or providing context to the generation model. However, focusing on retrieval parameters as the primary policy output in this GRPO context is more straightforward.\n",
        "# - For simplicity and direct control over a key retrieval parameter, let's define the output as a single numerical value representing `similarity_top_k`. The output layer will be a dense layer with 1 output neuron.\n",
        "\n",
        "# Therefore, the output of the policy network will be a tensor of shape (batch_size, 1), representing the predicted value for `similarity_top_k` for each question in the batch.\n",
        "\n",
        "# 3. Interpretation and Usage of the Policy Network's Output:\n",
        "# The output of the policy network (the predicted `similarity_top_k` value) will be used to configure the retrieval step of the RAG system for the given question.\n",
        "# - During training: The predicted `similarity_top_k` will be used to perform retrieval. The retrieved context, along with the question, will then be passed to the generation model to produce an answer. This answer will be compared to the ground truth to calculate the cosine similarity reward. This reward will be used by the GRPO algorithm to update the policy network's weights, encouraging it to predict `similarity_top_k` values that lead to higher rewards.\n",
        "# - During inference: The policy network will predict `similarity_top_k` for a new question, and this value will be used directly in the retrieval process to gather context for generating the final answer.\n",
        "# The predicted numerical output for `similarity_top_k` might need to be post-processed (e.g., rounded to an integer, clipped to a valid range) before being used by the RAG system's retriever."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ece352"
      },
      "source": [
        "## Implement the policy network\n",
        "\n",
        "### Subtask:\n",
        "Implement the policy network using a deep learning framework. This involves defining the transformer layers and the output layer(s) based on the input and output formats defined in the previous steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20adbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy network using PyTorch, defining the transformer layers and the output layer as specified in previous steps.\n",
        "\n",
        "In essence, this network takes a question, processes it through a pre-trained transformer to understand its context, and then uses a simple linear layer to predict a non-negative numerical value intended to represent the optimal similarity_top_k for retrieving documents for that question.\n",
        "\n",
        "In the context of Reinforcement Learning (RL), a policy network is a neural network that learns to map states (in our case, the user's question) to actions (the parameters or decisions that control the RAG system).\n",
        "\n",
        "**State:** The input is the user's question. The policy network processes this question to understand its meaning and context.\n",
        "\n",
        "**Action:** The output of the policy network is a value (or values) that influences how the RAG system operates. In the code we just discussed, the policy network's action space was initially simplified to predicting a single value: similarity_top_k, which determines how many relevant documents are retrieved. In the modified code, it predicts parameters for a distribution from which similarity_top_k is sampled.\n",
        "\n",
        "The goal of training the policy network using an algorithm like GRPO is to adjust its internal parameters (the weights and biases of the neural network) so that, when presented with a question, it predicts/samples actions (like a specific similarity_top_k) that lead to higher rewards (in our case, higher cosine similarity between the generated answer and the ground truth).\n",
        "\n",
        "So, the policy network's role is to learn the optimal strategy for configuring the RAG system based on the input question to maximize the desired outcome (answer quality, measured by the reward).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ff9198"
      },
      "source": [
        "# Task\n",
        "**Implement the policy update rule for the GRPO algorithm using cosine similarity as the reward signal to adjust the policy network's parameters in the provided notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a4c096"
      },
      "source": [
        "## Modify policy network output\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `RAGPolicyNetwork` to output parameters for a distribution over `similarity_top_k`, such as the mean and log-variance of a Gaussian distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fb1c21"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the RAGPolicyNetwork class to output parameters for a Gaussian distribution (mean and log-variance) over `similarity_top_k`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515fe81f"
      },
      "source": [
        "## Implement action sampling and log probability calculation\n",
        "\n",
        "### Subtask:\n",
        "Implement functions to sample `similarity_top_k` from the Gaussian distribution predicted by the policy network and calculate the log probability of the sampled action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71623969"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement functions to sample the action (similarity_top_k) from the predicted Gaussian distribution and calculate the log probability of the sampled action, using the predicted mean and log-variance from the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60f1e206"
      },
      "source": [
        "## Implement baseline calculation\n",
        "\n",
        "### Subtask:\n",
        "Create a function to calculate a baseline for the rewards, such as the average reward in a batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160b14ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to calculate the mean of a list or tensor of rewards to be used as a baseline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422375d8"
      },
      "source": [
        "## Set up training loop\n",
        "\n",
        "### Subtask:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8558acf5"
      },
      "source": [
        "**Reasoning**:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8479c2f2"
      },
      "source": [
        "## Implement policy update\n",
        "\n",
        "### Subtask:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using an optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0307182"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03bb1ac3"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "After training, evaluate the performance of the policy-controlled RAG system and refine the implementation or hyperparameters as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81edce18"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the policy-controlled RAG system after the training loop. This involves setting the policy network to evaluation mode, using a dataset (can be the same as training for demonstration), iterating through questions, predicting/sampling `similarity_top_k`, executing the RAG query, calculating the cosine similarity reward, and finally reporting the average reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287a48b1"
      },
      "source": [
        "# Task\n",
        "Implement observability metrics for the training process of a policy network using Weights and Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0da520a7"
      },
      "source": [
        "## Identify key metrics\n",
        "\n",
        "### Subtask:\n",
        "Determine which metrics are most important to track for monitoring the training process of the policy network (e.g., epoch number, batch number, average policy loss per batch, average reward per batch, average predicted similarity_top_k per batch, average advantage per batch).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e8b7ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify and list the key metrics for monitoring the training process of the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459dfae1"
      },
      "source": [
        "## Integrate weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Install the `wandb` library and initialize a Weights & Biases run at the beginning of the training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb8ada8"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the wandb library and initialize a Weights & Biases run at the beginning of the training script as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ca260e"
      },
      "source": [
        "## Log hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Log the training hyperparameters (e.g., learning rate, batch size, number of epochs) to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22da477d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a dictionary containing the training hyperparameters and log it to the initialized Weights & Biases run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de2b5e9"
      },
      "source": [
        "## Integrate metric calculation into the training loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to calculate the chosen metrics for each batch and/or epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9543160b"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to calculate the selected metrics for each batch and epoch, including average policy loss, average reward, average predicted similarity_top_k, average advantage, average mean, and average log-variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b06792f"
      },
      "source": [
        "## Log metrics to weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop. This will typically involve using `wandb.log()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48c0ca5"
      },
      "source": [
        "**Reasoning**:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop as requested by the subtask. This involves using `wandb.log()` for both batch and epoch metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbaad26"
      },
      "source": [
        "## Visualize and analyze metrics in weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Use the Weights & Biases dashboard to visualize the logged metrics, monitor training progress, and identify potential issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b145d0fd"
      },
      "source": [
        "## Refine logging and metrics\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked or the logging frequency for better insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d72db7"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked and their logging frequency in the training code. The current metrics are informative, but batch-level metrics can be noisy. Epoch-level metrics provide a smoother view of overall progress. We will keep logging both but ensure epoch metrics are clearly distinguished. We can also consider adding the standard deviation of `similarity_top_k` predictions at the epoch level to see how the policy's uncertainty evolves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ],
      "metadata": {
        "id": "vouyCtSmrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Q-1KQK72TSQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "xJw_mQJnTXkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "2hnq2QgOTi4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop includes the actual RAG execution using the CombinedFinancial10k_index and the cosine_similarity_reward function to calculate the reward based on the generated answer and the ground truth for each question in the batch. This means the policy network is being trained using real rewards derived from the RAG system's performance."
      ],
      "metadata": {
        "id": "y0UM0l3tbSZ5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4402a427"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # Explicitly import torch.nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torch.distributions import Normal\n",
        "import wandb\n",
        "import os # Import os to check for existing index\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, StorageContext, load_index_from_storage, Settings # Import necessary LlamaIndex components\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from google.colab import userdata\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoModel, AutoTokenizer # Import AutoModel and AutoTokenizer explicitly\n",
        "\n",
        "\n",
        "# Redefine necessary variables and functions from previous cells to ensure scope\n",
        "\n",
        "# Assuming OPENAI_API_KEY is already set as an environment variable in a previous cell\n",
        "# os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Setup OpenAI Model and Embeddings - Ensure these are set within this cell's execution\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024\n",
        "print(\"LlamaIndex Settings configured.\")\n",
        "\n",
        "# Assuming Google Drive is mounted at /content/drive and data_dir is defined\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "\n",
        "# Redefine get_index function to ensure it's available\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    print(f\"Index not found at {PERSIST_INDEX_DIR}{index_name}/. Creating index...\")\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    print(f\"Created and persisted index at {PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    print(f\"Loading index from storage at {PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    print(\"Loaded index from storage.\")\n",
        "\n",
        "  return index\n",
        "\n",
        "# Load or create the Combined Financial 10k index using the redefined get_index function\n",
        "# CombinedFinancial10k_index = get_index(\"CombinedFinancial10k\",f\"{data_dir}/RAG/data/10k/Combined_10k_Lyft_Uber.pdf\")\n",
        "\n",
        "\n",
        "\n",
        "# Redefine get_index function if needed (assuming index is persisted)\n",
        "# In this case, we will just load the index directly assuming it exists from previous runs\n",
        "# If you haven't run the cells to create and persist the index, you would need to do that first.\n",
        "\n",
        "# Load Combined Financial 10k index\n",
        "try:\n",
        "    # Correcting the persist_dir to match the intended index name and location\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}CombinedFinancial10k/\")\n",
        "    CombinedFinancial10k_index = load_index_from_storage(storage_context)\n",
        "    print(\"Loaded combined 10K index from storage.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Combined 10k index not found at {PERSIST_INDEX_DIR}CombinedFinancial10k/. Please run the cells to create and persist the index first.\")\n",
        "    # Handle the error, e.g., exit or create the index\n",
        "    CombinedFinancial10k_index = None # Set to None if not loaded\n",
        "\n",
        "# Redefine cosine_similarity_reward function\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Redefine sample_action_and_continuous function\n",
        "def sample_action_and_continuous(mean, log_variance):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    continuous_sample = distribution.sample()\n",
        "    processed_action = torch.max(torch.tensor(1.0), torch.round(torch.abs(continuous_sample)))\n",
        "    return processed_action, continuous_sample\n",
        "\n",
        "# Redefine calculate_baseline function\n",
        "def calculate_baseline(rewards):\n",
        "    if isinstance(rewards, list):\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    if rewards.numel() == 0:\n",
        "        return 0.0\n",
        "    return torch.mean(rewards)\n",
        "# log-probabilities of the actions the policy took in continuous action space(the agent chooses an action from a set of possible actions â€” this set is called the action space)\n",
        "def calculate_log_prob(mean, log_variance, action):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    log_prob = distribution.log_prob(action)\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "# Redefine questions and ground truth\n",
        "questions = [\"Please explain the revolving credit of Lyft for year 2023 and its long term benefits?\",\n",
        "    \"Could you give me a concise overview of the Business strategies employed by Uber and Lyft?\",\n",
        "    \"What are the details of Lyft's revolving credit for the year 2023 and how does it contribute to long-term benefits?\",\n",
        "    \"Could you analyze and compare the financial health of Uber and Lyft for the year 2023?\",\n",
        "    \"When compared to Uber is Lyft financially stable?\",\n",
        "    \"Can you compare the growth strategies between Uber and Lyft based on the 10K document for year 2023?\",\n",
        "    \"Can you provide a summary of the Financial and Operational Highlights of Uber?\"\n",
        "    \"How would you describe the Business overview of Uber and Lyft in a nutshell?\",\n",
        "    \"How does Lyft's financial stability in 2023 compare to Uber's?\",\n",
        "    \"Can you provide an overview of Lyft's revolving credit in 2023 and its potential long-term advantages?\",\n",
        "    \"Can you please compare the financial status for both the companies for the year 2023?\",\n",
        "    \"Can you compare the cost and expenses between Lyft and Uber for year 2023? Also let me know which company faired better?\",\n",
        "    \"Can you provide a financial comparison between Uber and Lyft for the year 2023?\",\n",
        "    \"Could you give me an overview of Lyft's Financial and Operational performance?\",\n",
        "    \"In terms of financial stability, how does Lyft stack up against Uber for the year 2023?\",\n",
        "    \"How does Lyft's revolving credit in 2023 work and what are the advantages it offers in the long run?\",\n",
        "    \"When it comes to technology which one fairs better is it Uber or Lyft?\",\n",
        "    \"What are the contrasting growth approaches of Uber and Lyft as outlined in the 2023 10K report?\",\n",
        "    \"Could you give me a concise overview of the Business strategies employed by Uber and Lyft?\",\n",
        "    \"Could you analyze and compare the financial health of Uber and Lyft for the year 2023?\",\n",
        "    \"Can you summarize the important Financial and Operational metrics for Lyft\",\n",
        "    \"When evaluating financial stability, how do Uber and Lyft differ in their performance for the year 2023?\",]\n",
        "ground_truth = [\"In 2023, Lyft entered into a revolving credit agreement with certain lenders for a 1.25 billion.  The long-term benefits of this revolving credit facility include providing Lyft with financial flexibility and access to capital when needed. It allows Lyft to borrow funds, repay them, and borrow again up to the agreed-upon limit, providing liquidity for operational needs, investments, or other strategic initiatives. Additionally, having a revolving credit facility in place can help Lyft manage its cash flow, take advantage of opportunities for growth, and navigate any unforeseen financial challenges.\",\n",
        "    \"Based on the information provided in the 10K documents for 2023, both Uber and Lyft have growth strategies focused on increasing their user base, expanding their range of transportation services, and capturing a larger share of the transportation market. Lyft's growth strategy includes increasing rider use cases by offering various products and services such as Lyft Pink subscription plan, Lyft Pass commuter programs, and first-mile and last-mile services. They also aim to grow their share of consumers' transportation spend by providing a wide range of mobility options through their rideshare, bikes, and scooters network.On the other hand, Uber's growth strategy involves using their technology platform to connect consumers with various ride services, merchants, and delivery service providers. They also connect consumers with public transportation networks and provide solutions in the freight industry. Uber is also developing new technologies to solve everyday problems.Both companies face competition from other players in the market, and they are focused on innovation, technology, and expanding their services to stay competitive and attract more users.\",\n",
        "    \"In 2023, Lyft entered into a revolving credit agreement with certain lenders for a 1.25 billion.  The long-term benefits of this revolving credit facility include providing Lyft with financial flexibility and access to capital when needed. It allows Lyft to borrow funds, repay them, and borrow again up to the agreed-upon limit, providing liquidity for operational needs, investments, or other strategic initiatives. Additionally, having a revolving credit facility in place can help Lyft manage its cash flow, take advantage of opportunities for growth, and navigate any unforeseen financial challenges.\",\n",
        "    \"Financial Highlights: Lyft, Inc. operates multimodal transportation networks in the United States and Canada. The company's revenue is primarily generated from its ridesharing marketplace connecting drivers and riders. Lyft collects service fees and commissions from drivers for their use of the ridesharing marketplace. G ross Bookings and Rides increase as drivers accept more rider leads.Operational Highlights: Lyft offers an expanded set of transportation modes in select cities, including shared bikes and scooters for shorter rides and multimodal trips. The company's platform and mobile-based applications facilitate peer-to-peer ridesharing by connecting drivers with riders. Lyft aims to grow its share of consumers' transportation spend by providing a wide range of mobility options. The company focuses on delivering increasing value to drivers by offering economic opportunities and programs like Express Drive for access to rental cars.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"Based on the information provided in the 10K documents for 2023, both Uber and Lyft have growth strategies focused on increasing their user base, expanding their range of transportation services, and capturing a larger share of the transportation market. Lyft's growth strategy includes increasing rider use cases by offering various products and services such as Lyft Pink subscription plan, Lyft Pass commuter programs, and first-mile and last-mile services. They also aim to grow their share of consumers' transportation spend by providing a wide range of mobility options through their rideshare, bikes, and scooters network.On the other hand, Uber's growth strategy involves using their technology platform to connect consumers with various ride services, merchants, and delivery service providers. They also connect consumers with public transportation networks and provide solutions in the freight industry. Uber is also developing new technologies to solve everyday problems.Both companies face competition from other players in the market, and they are focused on innovation, technology, and expanding their services to stay competitive and attract more users.\",\n",
        "    \"Financial Highlights: Lyft, Inc. operates multimodal transportation networks in the United States and Canada. The company's revenue is primarily generated from its ridesharing marketplace connecting drivers and riders. Lyft collects service fees and commissions from drivers for their use of the ridesharing marketplace. G ross Bookings and Rides increase as drivers accept more rider leads.Operational Highlights: Lyft offers an expanded set of transportation modes in select cities, including shared bikes and scooters for shorter rides and multimodal trips. The company's platform and mobile-based applications facilitate peer-to-peer ridesharing by connecting drivers with riders. Lyft aims to grow its share of consumers' transportation spend by providing a wide range of mobility options. The company focuses on delivering increasing value to drivers by offering economic opportunities and programs like Express Drive for access to rental cars.\",\n",
        "    \"Lyft, Inc. started a movement to revolutionize transportation in 2012 with a peer-to-peer marketplace for on-demand ridesharing. They have continued to pioneer innovations and are now one of the largest multimodal transportation networks in the United States and Canada. Lyft's purpose is to get riders out into the world and provide drivers with control over their time and money. They offer a ridesharing marketplace through the Lyft App, connecting drivers with riders and providing various transportation modes. Uber Technologies, Inc. is a technology platform that powers movement from point A to point B. They connect consumers with providers of ride services, merchants, and delivery service providers. Uber also connects consumers with public transportation networks and carriers in the freight industry. They are developing technologies to provide new solutions for everyday problems.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"In 2023, Lyft entered into a revolving credit agreement with certain lenders for a 1.25 billion.  The long-term benefits of this revolving credit facility include providing Lyft with financial flexibility and access to capital when needed. It allows Lyft to borrow funds, repay them, and borrow again up to the agreed-upon limit, providing liquidity for operational needs, investments, or other strategic initiatives. Additionally, having a revolving credit facility in place can help Lyft manage its cash flow, take advantage of opportunities for growth, and navigate any unforeseen financial challenges.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"In 2023, Lyft reported total costs and expenses of 36,171 million. Comparing the two, Lyft had higher costs and expenses than Uber for the year 2023. Therefore, Uber fared better in terms of managing costs and expenses in 2023.\"\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"Financial Highlights: Lyft, Inc. operates multimodal transportation networks in the United States and Canada. The company's revenue is primarily generated from its ridesharing marketplace connecting drivers and riders. Lyft collects service fees and commissions from drivers for their use of the ridesharing marketplace. G ross Bookings and Rides increase as drivers accept more rider leads.Operational Highlights: Lyft offers an expanded set of transportation modes in select cities, including shared bikes and scooters for shorter rides and multimodal trips. The company's platform and mobile-based applications facilitate peer-to-peer ridesharing by connecting drivers with riders. Lyft aims to grow its share of consumers' transportation spend by providing a wide range of mobility options. The company focuses on delivering increasing value to drivers by offering economic opportunities and programs like Express Drive for access to rental cars.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"In 2023, Lyft entered into a revolving credit agreement with certain lenders for a 1.25 billion.  The long-term benefits of this revolving credit facility include providing Lyft with financial flexibility and access to capital when needed. It allows Lyft to borrow funds, repay them, and borrow again up to the agreed-upon limit, providing liquidity for operational needs, investments, or other strategic initiatives. Additionally, having a revolving credit facility in place can help Lyft manage its cash flow, take advantage of opportunities for growth, and navigate any unforeseen financial challenges.\",\n",
        "    \"Based on the provided context, both Uber and Lyft heavily rely on technology to power their transportation services. Uber is described as a technology platform that uses leading technology to connect consumers with ride services, delivery services, and public transportation networks. Lyft, on the hand, leverages its technology platform to connect drivers with riders through its ridesharing marketplace and offers an expanded set of transportation modes. In terms of technology, both Uber and Lyft have their strengths and focus areas. Uber emphasizes its technology applications supporting a variety of offerings on its platform, while Lyft highlights its robust technology platform that powers rides and connections every day. Ultimately, the effectiveness and success of each company's technology may vary based on specific features, innovations, and user experiences.\",\n",
        "    \"Based on the information provided in the 10K documents for 2023, both Uber and Lyft have growth strategies focused on increasing their user base, expanding their range of transportation services, and capturing a larger share of the transportation market. Lyft's growth strategy includes increasing rider use cases by offering various products and services such as Lyft Pink subscription plan, Lyft Pass commuter programs, and first-mile and last-mile services. They also aim to grow their share of consumers' transportation spend by providing a wide range of mobility options through their rideshare, bikes, and scooters network.On the other hand, Uber's growth strategy involves using their technology platform to connect consumers with various ride services, merchants, and delivery service providers. They also connect consumers with public transportation networks and provide solutions in the freight industry. Uber is also developing new technologies to solve everyday problems.Both companies face competition from other players in the market, and they are focused on innovation, technology, and expanding their services to stay competitive and attract more users.\",\n",
        "    \"Lyft, Inc. started a movement to revolutionize transportation in 2012 with a peer-to-peer marketplace for on-demand ridesharing. They have continued to pioneer innovations and are now one of the largest multimodal transportation networks in the United States and Canada. Lyft's purpose is to get riders out into the world and provide drivers with control over their time and money. They offer a ridesharing marketplace through the Lyft App, connecting drivers with riders and providing various transportation modes.Uber Technologies, Inc. is a technology platform that powers movement from point A to point B. They connect consumers with providers of ride services, merchants, and delivery service providers. Uber also connects consumers with public transportation networks and carriers in the freight industry. They are developing new technologies to provide new solutions for everyday problems.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\",\n",
        "    \"Financial Highlights: Lyft, Inc. operates multimodal transportation networks in the United States and Canada. The company's revenue is primarily generated from its ridesharing marketplace connecting drivers and riders. Lyft collects service fees and commissions from drivers for their use of the ridesharing marketplace. Gross Bookings and Rides increase as drivers accept more rider leads.Operational Highlights: Lyft offers an expanded set of transportation modes in select cities, including shared bikes and scooters for shorter rides and multimodal trips. The company's platform and mobile-based applications facilitate peer-to-peer ridesharing by connecting drivers with riders. Lyft aims to grow its share of consumers' transportation spend by providing a wide range of mobility options. The company focuses on delivering increasing value to drivers by offering economic opportunities and programs like Express Drive for access to rental cars.\",\n",
        "    \"Based on the provided context, it appears that Lyft is in a better financial position compared to Uber for the year 2023. Lyft reported revenue of 37,281 million. Additionally, Lyft's net loss percentage was 33.1% compared to Uber's net income percentage of 5%. This indicates that Lyft had a lower loss percentage compared to Uber's income percentage, suggesting that Lyft may be more financially stable in 2023.\"\n",
        "    ]\n",
        "\n",
        "# Redefine RAGPolicyNetwork class\n",
        "class RAGPolicyNetwork(nn.Module):\n",
        "    def __init__(self, transformer_model_name=\"bert-base-uncased\", output_dim=2):\n",
        "        super(RAGPolicyNetwork, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
        "        transformer_output_dim = self.transformer.config.hidden_size\n",
        "        self.output_layer = nn.Linear(transformer_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        encoded_input = self.tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
        "        outputs = self.transformer(**encoded_input)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        mean_and_log_variance = self.output_layer(pooled_output)\n",
        "        mean = mean_and_log_variance[:, 0]\n",
        "        log_variance = mean_and_log_variance[:, 1]\n",
        "        return mean, log_variance\n",
        "\n",
        "# Redefine sample_action_and_continuous function\n",
        "def sample_action_and_continuous(mean, log_variance):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    continuous_sample = distribution.sample()\n",
        "    processed_action = torch.max(torch.tensor(1.0), torch.round(torch.abs(continuous_sample)))\n",
        "    return processed_action, continuous_sample\n",
        "\n",
        "# Redefine calculate_baseline function\n",
        "def calculate_baseline(rewards):\n",
        "    if isinstance(rewards, list):\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    if rewards.numel() == 0:\n",
        "        return 0.0\n",
        "    return torch.mean(rewards)\n",
        "\n",
        "def calculate_log_prob(mean, log_variance, action):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    log_prob = distribution.log_prob(action)\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "# Redefine Dataset and DataLoader\n",
        "class RAGDataset(Dataset):\n",
        "    def __init__(self, questions, ground_truth):\n",
        "        self.questions = questions\n",
        "        self.ground_truth = ground_truth\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.ground_truth[idx]\n",
        "\n",
        "# Define dataset, dataloader, and training parameters if not already\n",
        "if 'rag_dataset' not in globals():\n",
        "    rag_dataset = RAGDataset(questions, ground_truth)\n",
        "if 'BATCH_SIZE' not in globals():\n",
        "    BATCH_SIZE = 8\n",
        "if 'train_dataloader' not in globals():\n",
        "    train_dataloader = DataLoader(rag_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "if 'NUM_EPOCHS' not in globals():\n",
        "    NUM_EPOCHS = 5\n",
        "if 'LEARNING_RATE' not in globals():\n",
        "    LEARNING_RATE = 1e-4\n",
        "\n",
        "# Re-instantiate policy_group and optimizers if not available or size mismatch\n",
        "if 'policy_group' not in globals() or len(policy_group) != 5: # Assuming NUM_POLICIES = 5\n",
        "    NUM_POLICIES = 5 # Ensure NUM_POLICIES is defined\n",
        "    print(f\"Policy group not found or size mismatch. Re-instantiating {NUM_POLICIES} policies.\")\n",
        "    policy_group = nn.ModuleList()\n",
        "    for i in range(NUM_POLICIES):\n",
        "        policy = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "        policy_group.append(policy)\n",
        "    optimizers = [optim.Adam(policy.parameters(), lr=LEARNING_RATE) for policy in policy_group]\n",
        "    print(f\"Re-instantiated a group of {NUM_POLICIES} RAGPolicyNetwork instances and optimizers.\")\n",
        "else:\n",
        "    print(f\"Policy group with {len(policy_group)} instances and optimizers already exists.\")\n",
        "\n",
        "\n",
        "# Initialize a Weights & Biases run (if not already initialized and active)\n",
        "if wandb.run is None:\n",
        "    try:\n",
        "        if 'wandb' not in globals(): import wandb\n",
        "        wandb.init(project=\"rag-policy-training\", name=\"grpo-cosine-similarity-group-logging\", reinit=True)\n",
        "        config = {\n",
        "            \"learning_rate\": LEARNING_RATE, \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS,\n",
        "            \"transformer_model\": \"bert-base-uncased\", \"output_dim\": 2, \"num_policies\": len(policy_group)\n",
        "        }\n",
        "        wandb.config.update(config)\n",
        "        print(\"Training hyperparameters logged to Weights & Biases config.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Weights & Biases: {e}\")\n",
        "        print(\"Weights & Biases logging will be skipped.\")\n",
        "elif wandb.run is not None:\n",
        "    print(f\"Weights & Biases run '{wandb.run.name}' is already active.\")\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"Starting policy group training with policy evaluation and logging...\")\n",
        "\n",
        "if 'global_step' not in globals(): global_step = 0\n",
        "\n",
        "# Check if CombinedFinancial10k_index was loaded successfully before starting training\n",
        "# Corrected check to use CombinedFinancial10k_index\n",
        "if CombinedFinancial10k_index is not None:\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Data structures to collect data across policies for this iteration/epoch\n",
        "        all_policy_rewards = {}\n",
        "        all_policy_continuous_samples = {} # Store continuous samples for gradient tracking\n",
        "        all_policy_sampled_k_processed = {}\n",
        "        # Removed all_policy_advantages storage during data collection as it will be recalculated\n",
        "        all_policy_means = {}\n",
        "        all_policy_log_variances = {}\n",
        "        all_policy_losses = {}\n",
        "        all_policy_questions = {} # Store questions for re-running forward pass\n",
        "\n",
        "        # --- Data Collection Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Collecting data...\")\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy.train()\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            all_policy_rewards[policy_name] = []\n",
        "            all_policy_continuous_samples[policy_name] = [] # Initialize storage for continuous samples\n",
        "            all_policy_sampled_k_processed[policy_name] = []\n",
        "            all_policy_means[policy_name] = []\n",
        "            all_policy_log_variances[policy_name] = []\n",
        "            all_policy_losses[policy_name] = []\n",
        "            all_policy_questions[policy_name] = [] # Initialize storage for questions\n",
        "\n",
        "            for batch_idx, (batch_questions, batch_ground_truth) in enumerate(train_dataloader):\n",
        "                if not batch_questions: continue\n",
        "\n",
        "                # Ensure gradients are tracked for policy outputs\n",
        "                mean_output, log_variance_output = policy(list(batch_questions))\n",
        "\n",
        "                batch_sampled_k_processed = []\n",
        "                batch_sampled_k_continuous = []\n",
        "                batch_rewards = []\n",
        "\n",
        "                for i in range(len(batch_questions)):\n",
        "                    # sample_action_and_continuous function already returns tensors\n",
        "                    sampled_k_processed_item, sampled_k_continuous_item = sample_action_and_continuous(mean_output[i], log_variance_output[i])\n",
        "                    batch_sampled_k_processed.append(sampled_k_processed_item.detach()) # Detach processed_action for plotting/logging\n",
        "                    batch_sampled_k_continuous.append(sampled_k_continuous_item) # Keep continuous_sample attached to graph\n",
        "\n",
        "                    question = batch_questions[i]\n",
        "                    ground_truth_answer = batch_ground_truth[i]\n",
        "                    predicted_top_k_int = max(1, int(sampled_k_processed_item.item()))\n",
        "\n",
        "                    try:\n",
        "                        # Use CombinedFinancial10k_index here\n",
        "                        policy_controlled_engine = CombinedFinancial10k_index.as_query_engine(similarity_top_k=predicted_top_k_int)\n",
        "                        generated_answer = policy_controlled_engine.query(question).response\n",
        "                        reward = cosine_similarity_reward(generated_answer, ground_truth_answer)\n",
        "                        batch_rewards.append(reward)\n",
        "                    except Exception as e:\n",
        "                        # print(f\"    Error during RAG execution or reward calculation for question '{question}': {e}\")\n",
        "                        batch_rewards.append(0.0)\n",
        "\n",
        "                if not batch_rewards: batch_rewards_tensor = torch.tensor([], dtype=torch.float32)\n",
        "                else: batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "\n",
        "                # batch_sampled_k_continuous list contains tensors that are part of the graph\n",
        "                if not batch_sampled_k_continuous: batch_sampled_k_continuous_tensor = torch.tensor([], dtype=torch.float32)\n",
        "                else: batch_sampled_k_continuous_tensor = torch.stack(batch_sampled_k_continuous)\n",
        "\n",
        "\n",
        "                all_policy_rewards[policy_name].extend(batch_rewards_tensor.tolist())\n",
        "                all_policy_sampled_k_processed[policy_name].extend([k.item() for k in batch_sampled_k_processed])\n",
        "                all_policy_continuous_samples[policy_name].extend(batch_sampled_k_continuous) # Store continuous samples\n",
        "\n",
        "                # Calculate log probabilities for the batch - needs to be on tensors that track gradients\n",
        "                # We will recalculate log_probs in the update step using the stored continuous samples\n",
        "                # and a fresh forward pass, so no need to store log_probs here.\n",
        "\n",
        "                all_policy_means[policy_name].extend(mean_output.detach().tolist()) # Detach for storage/logging\n",
        "                all_policy_log_variances[policy_name].extend(log_variance_output.detach().tolist()) # Detach for storage/logging\n",
        "\n",
        "                # Store questions for re-running forward pass in update step\n",
        "                all_policy_questions[policy_name].extend(list(batch_questions))\n",
        "\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        # --- Implement Group Performance Evaluation and Logging ---\n",
        "        policy_avg_rewards = {}\n",
        "        best_policy_name = None\n",
        "        highest_avg_reward = -float('inf')\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Evaluating policy performance and logging epoch metrics...\")\n",
        "        epoch_metrics = {}\n",
        "        group_avg_reward = 0.0\n",
        "        total_valid_rewards = 0\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            epoch_rewards = all_policy_rewards[policy_name]\n",
        "            avg_epoch_reward = np.mean(epoch_rewards) if epoch_rewards else 0.0\n",
        "            policy_avg_rewards[policy_name] = avg_epoch_reward\n",
        "            if avg_epoch_reward > highest_avg_reward:\n",
        "                highest_avg_reward = avg_epoch_reward\n",
        "                best_policy_name = policy_name\n",
        "            epoch_predicted_k = all_policy_sampled_k_processed[policy_name]\n",
        "            # Advantages are not stored here anymore, calculate average advantage if needed for logging from update step\n",
        "            epoch_means = all_policy_means[policy_name]\n",
        "            epoch_log_variances = all_policy_log_variances[policy_name]\n",
        "            avg_epoch_predicted_top_k = np.mean(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            epoch_predicted_top_k_std = np.std(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            avg_epoch_mean = np.mean(epoch_means) if epoch_means else 0\n",
        "            avg_epoch_log_variance = np.mean(epoch_log_variances) if epoch_log_variances else 0\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_reward\"] = avg_epoch_reward\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_predicted_top_k\"] = avg_epoch_predicted_top_k\n",
        "            epoch_metrics[f\"{policy_name}/epoch_predicted_top_k_std\"] = epoch_predicted_top_k_std\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_mean\"] = avg_epoch_mean\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_log_variance\"] = avg_epoch_log_variance\n",
        "            group_avg_reward += np.sum(epoch_rewards)\n",
        "            total_valid_rewards += len(epoch_rewards)\n",
        "            print(f\"    {policy_name}: Avg Reward = {avg_epoch_reward:.4f}, Avg Predicted Top K = {avg_epoch_predicted_top_k:.2f}, Predicted Top K Std = {epoch_predicted_top_k_std:.2f}\")\n",
        "\n",
        "        group_avg_reward = group_avg_reward / total_valid_rewards if total_valid_rewards > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Best performing policy is {best_policy_name} with Avg Reward = {highest_avg_reward:.4f}\")\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Group Average Reward = {group_avg_reward:.4f}\")\n",
        "        epoch_metrics[\"epoch/best_policy\"] = best_policy_name\n",
        "        epoch_metrics[\"epoch/highest_avg_reward\"] = highest_avg_reward\n",
        "        epoch_metrics[\"epoch/group_average_reward\"] = group_avg_reward\n",
        "        wandb.log(epoch_metrics, step=epoch + 1)\n",
        "\n",
        "        # --- Implement Policy Update Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Starting policy update...\")\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            optimizer = optimizers[policy_idx]\n",
        "\n",
        "            # Retrieve collected data (rewards, continuous samples, and questions)\n",
        "            policy_rewards_list = all_policy_rewards[policy_name]\n",
        "            policy_continuous_samples = all_policy_continuous_samples[policy_name] # List of tensors\n",
        "            policy_questions = all_policy_questions[policy_name] # List of strings\n",
        "\n",
        "            # Convert rewards to tensor (doesn't need grad for standard policy gradient)\n",
        "            policy_rewards = torch.tensor(policy_rewards_list, dtype=torch.float32)\n",
        "\n",
        "            # Recalculate advantages using the collected rewards\n",
        "            baseline = calculate_baseline(policy_rewards) # Calculate baseline from collected rewards\n",
        "            policy_advantages = policy_rewards - baseline # Calculate advantages\n",
        "\n",
        "            # Re-run forward pass on the stored questions to get log_probs that track gradients\n",
        "            if not policy_questions:\n",
        "                 print(f\"    {policy_name}: No questions available for update.\")\n",
        "                 wandb.log({f\"{policy_name}/policy_loss\": 0.0,}, step=epoch + 1)\n",
        "                 continue # Skip update if no questions\n",
        "\n",
        "            # Ensure policy is in training mode before forward pass for update\n",
        "            policy.train()\n",
        "            mean_output_update, log_variance_output_update = policy(policy_questions)\n",
        "\n",
        "            # Stack the collected continuous samples into a single tensor\n",
        "            if not policy_continuous_samples:\n",
        "                 print(f\"    {policy_name}: No continuous samples available for update.\")\n",
        "                 wandb.log({f\"{policy_name}/policy_loss\": 0.0,}, step=epoch + 1)\n",
        "                 continue # Skip update if no samples\n",
        "\n",
        "            policy_continuous_samples_tensor = torch.stack(policy_continuous_samples)\n",
        "\n",
        "            # Recalculate log probabilities using the fresh policy outputs and the stored continuous samples\n",
        "            policy_log_probs = calculate_log_prob(mean_output_update, log_variance_output_update, policy_continuous_samples_tensor)\n",
        "\n",
        "\n",
        "            # Filter out samples where original reward was 0 (likely due to errors in RAG execution)\n",
        "            # We use the original rewards to determine which samples were valid.\n",
        "            valid_indices = policy_rewards != 0\n",
        "            if torch.sum(valid_indices) > 0:\n",
        "                valid_log_probs = policy_log_probs[valid_indices]\n",
        "                valid_advantages = policy_advantages[valid_indices]\n",
        "\n",
        "                # Calculate the policy loss using the valid log probabilities and advantages\n",
        "                policy_loss = -torch.mean(valid_log_probs * valid_advantages)\n",
        "\n",
        "                # Perform optimizer.zero_grad() for the current policy's optimizer\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Call policy_loss.backward() to compute gradients\n",
        "                policy_loss.backward()\n",
        "\n",
        "                # Call optimizer.step() to update the current policy's parameters\n",
        "                optimizer.step()\n",
        "\n",
        "                # Log the policy loss for each policy after its update\n",
        "                wandb.log({\n",
        "                    f\"{policy_name}/policy_loss\": policy_loss.item(),\n",
        "                }, step=epoch + 1)\n",
        "\n",
        "            else:\n",
        "                wandb.log({\n",
        "                    f\"{policy_name}/policy_loss\": 0.0, # Log 0 loss if no valid samples for update\n",
        "                }, step=epoch + 1)\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Policy update completed.\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training skipped because CombinedFinancial10k index was not loaded.\")\n",
        "\n",
        "# Finish the Weights & Biases run\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of training results logged in W&B**\n",
        "\n",
        "**Epoch 27/100, Avg Loss: 0.0183, Avg Reward: 0.3199, Avg Predicted Top K: 2.80, Predicted Top K Std: 0.40**\n",
        "\n",
        "**Epoch** 27/100: This indicates that the training is currently on the 27th iteration (epoch) out of a planned total of 100 epochs.\n",
        "\n",
        "**Avg Loss**: 0.0183: This is the average policy loss calculated across all the batches in Epoch 27. In reinforcement learning, the policy loss is a value that the optimization algorithm (like the one used in GRPO) tries to minimize. A **lower loss** generally suggests that the **policy** is being **updated** in a **direction** that is expected to **increase** **rewards**. The value 0.0183 is the magnitude of this average loss for this specific epoch.\n",
        "\n",
        "**Avg Reward**: 0.3199: This is the **average cosine similarity reward** obtained during Epoch 27. This reward is calculated by running the RAG system with the actions (predicted similarity_top_k values) sampled by the policy network for each question in the dataset and then comparing the generated answers to the ground truth. An **average** **reward** of 0.3199 means that, on **average** across all samples in this epoch, the **generated** **answers** had a **cosine** **similarity** of approximately 0.32 with their **respective** **ground** **truth** **answers**. **Higher** values indicate **better** **performance** in terms of **semantic** **similarity**.\n",
        "\n",
        "**Avg Predicted Top K**: 2.80: This is the average value of similarity_top_k predicted (or sampled and then processed into an integer) by the policy network across all questions in Epoch 27. This **metric** **tells** you what the **policy** **network** is generally **choosing** for the **number** of **documents** to **retrieve**. An **average** of 2.80 suggests the **policy** is typically **selecting** around 3 **documents**.\n",
        "\n",
        "**Predicted Top K Std**: 0.40: This is the standard deviation of the similarity_top_k values predicted (or sampled and processed) by the policy network across all questions in Epoch 27. The standard deviation measures the dispersion or spread of the predicted values. A standard deviation of 0.40 indicates that the predicted similarity_top_k values in this epoch were relatively close to the average (2.80), meaning the policy's predictions for similarity_top_k didn't vary widely within this epoch."
      ],
      "metadata": {
        "id": "JpnYg3dccV0v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de918f7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The training process successfully logged key metrics at both the batch and epoch levels to Weights & Biases, including policy loss, average reward, average predicted `similarity_top_k`, average advantage, average mean, and average log-variance of the predicted distribution.\n",
        "*   Hyperparameters such as learning rate (1e-4), batch size (8), and number of epochs (100) were successfully logged to the Weights & Biases config.\n",
        "*   The training loop executed for 100 epochs, with console output confirming the progress and epoch-average loss and reward.\n",
        "*   A new epoch-level metric, the standard deviation of the predicted `similarity_top_k`, was successfully added and logged to provide insight into the variability of the policy's actions.\n",
        "\n",
        "### Completed Steps\n",
        "\n",
        "*   Visualized the logged metrics in the Weights & Biases dashboard to analyze trends, identify correlations between metrics (e.g., reward and predicted top\\_k), and diagnose potential training issues such as instability or convergence problems.\n",
        "*   Implement the actual RAG system reward calculation to replace the dummy reward function, allowing the policy to learn based on real retrieval performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53e9c748"
      },
      "source": [
        "# Task\n",
        "Implement the true GRPO update mechanism in the provided notebook, incorporating a reference policy, policy divergence calculation, and a trust region or penalty based on the best performing policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40aea025"
      },
      "source": [
        "## Define a reference policy\n",
        "\n",
        "### Subtask:\n",
        "Establish a reference policy, which is typically the average policy across the group or the best-performing policy from the previous iteration. This will serve as a benchmark for calculating policy divergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0e25678"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the first instruction of the subtask to establish a reference policy by identifying the best performing policy's state dictionary at the beginning of each epoch and handling the initial epoch case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwR4paWYFk3"
      },
      "source": [
        "# --- Training Loop ---\n",
        "print(\"Starting policy group training with policy evaluation and logging...\")\n",
        "\n",
        "if 'global_step' not in globals(): global_step = 0\n",
        "# Initialize reference_policy_state_dict before the loop\n",
        "reference_policy_state_dict = None\n",
        "\n",
        "# Check if CombinedFinancial10k_index was loaded successfully before starting training\n",
        "if CombinedFinancial10k_index is not None:\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Data structures to collect data across policies for this iteration/epoch\n",
        "        all_policy_rewards = {}\n",
        "        all_policy_continuous_samples = {} # Store continuous samples for gradient tracking\n",
        "        all_policy_sampled_k_processed = {}\n",
        "        all_policy_means = {}\n",
        "        all_policy_log_variances = {}\n",
        "        all_policy_losses = {}\n",
        "        all_policy_questions = {} # Store questions for re-running forward pass\n",
        "\n",
        "        # --- Data Collection Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Collecting data...\")\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy.train()\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            all_policy_rewards[policy_name] = []\n",
        "            all_policy_continuous_samples[policy_name] = [] # Initialize storage for continuous samples\n",
        "            all_policy_sampled_k_processed[policy_name] = []\n",
        "            all_policy_means[policy_name] = []\n",
        "            all_policy_log_variances[policy_name] = []\n",
        "            all_policy_losses[policy_name] = []\n",
        "            all_policy_questions[policy_name] = [] # Initialize storage for questions\n",
        "\n",
        "            for batch_idx, (batch_questions, batch_ground_truth) in enumerate(train_dataloader):\n",
        "                if not batch_questions: continue\n",
        "\n",
        "                # Ensure gradients are tracked for policy outputs\n",
        "                mean_output, log_variance_output = policy(list(batch_questions))\n",
        "\n",
        "                batch_sampled_k_processed = []\n",
        "                batch_sampled_k_continuous = []\n",
        "                batch_rewards = []\n",
        "\n",
        "                for i in range(len(batch_questions)):\n",
        "                    # sample_action_and_continuous function already returns tensors\n",
        "                    sampled_k_processed_item, sampled_k_continuous_item = sample_action_and_continuous(mean_output[i], log_variance_output[i])\n",
        "                    batch_sampled_k_processed.append(sampled_k_processed_item.detach()) # Detach processed_action for plotting/logging\n",
        "                    batch_sampled_k_continuous.append(sampled_k_continuous_item) # Keep continuous_sample attached to graph\n",
        "\n",
        "                    question = batch_questions[i]\n",
        "                    ground_truth_answer = batch_ground_truth[i]\n",
        "                    predicted_top_k_int = max(1, int(sampled_k_processed_item.item()))\n",
        "\n",
        "                    try:\n",
        "                        # Use CombinedFinancial10k_index here\n",
        "                        policy_controlled_engine = CombinedFinancial10k_index.as_query_engine(similarity_top_k=predicted_top_k_int)\n",
        "                        generated_answer = policy_controlled_engine.query(question).response\n",
        "                        reward = cosine_similarity_reward(generated_answer, ground_truth_answer)\n",
        "                        batch_rewards.append(reward)\n",
        "                    except Exception as e:\n",
        "                        # print(f\"    Error during RAG execution or reward calculation for question '{question}': {e}\")\n",
        "                        batch_rewards.append(0.0)\n",
        "\n",
        "                if not batch_rewards: batch_rewards_tensor = torch.tensor([], dtype=torch.float32)\n",
        "                else: batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "\n",
        "                # batch_sampled_k_continuous list contains tensors that are part of the graph\n",
        "                if not batch_sampled_k_continuous: batch_sampled_k_continuous_tensor = torch.tensor([], dtype=torch.float32)\n",
        "                else: batch_sampled_k_continuous_tensor = torch.stack(batch_sampled_k_continuous)\n",
        "\n",
        "\n",
        "                all_policy_rewards[policy_name].extend(batch_rewards_tensor.tolist())\n",
        "                all_policy_sampled_k_processed[policy_name].extend([k.item() for k in batch_sampled_k_processed])\n",
        "                all_policy_continuous_samples[policy_name].extend(batch_sampled_k_continuous) # Store continuous samples\n",
        "\n",
        "                # Calculate log probabilities for the batch - needs to be on tensors that track gradients\n",
        "                # We will recalculate log_probs in the update step using the stored continuous samples\n",
        "                # and a fresh forward pass, so no need to store log_probs here.\n",
        "\n",
        "                all_policy_means[policy_name].extend(mean_output.detach().tolist()) # Detach for storage/logging\n",
        "                all_policy_log_variances[policy_name].extend(log_variance_output.detach().tolist()) # Detach for storage/logging\n",
        "\n",
        "                # Store questions for re-running forward pass in update step\n",
        "                all_policy_questions[policy_name].extend(list(batch_questions))\n",
        "\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        # --- Implement Group Performance Evaluation and Logging ---\n",
        "        policy_avg_rewards = {}\n",
        "        best_policy_name = None\n",
        "        highest_avg_reward = -float('inf')\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Evaluating policy performance and logging epoch metrics...\")\n",
        "        epoch_metrics = {}\n",
        "        group_avg_reward = 0.0\n",
        "        total_valid_rewards = 0\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            epoch_rewards = all_policy_rewards[policy_name]\n",
        "            avg_epoch_reward = np.mean(epoch_rewards) if epoch_rewards else 0.0\n",
        "            policy_avg_rewards[policy_name] = avg_epoch_reward\n",
        "            if avg_epoch_reward > highest_avg_reward:\n",
        "                highest_avg_reward = avg_epoch_reward\n",
        "                best_policy_name = policy_name\n",
        "            epoch_predicted_k = all_policy_sampled_k_processed[policy_name]\n",
        "            epoch_means = all_policy_means[policy_name]\n",
        "            epoch_log_variances = all_policy_log_variances[policy_name]\n",
        "            avg_epoch_predicted_top_k = np.mean(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            # Calculate standard deviation of predicted top_k here\n",
        "            avg_epoch_predicted_top_k_std = np.std(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            avg_epoch_mean = np.mean(epoch_means) if epoch_means else 0\n",
        "            avg_epoch_log_variance = np.mean(epoch_log_variances) if epoch_log_variances else 0\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_reward\"] = avg_epoch_reward\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_predicted_top_k\"] = avg_epoch_predicted_top_k\n",
        "            epoch_metrics[f\"{policy_name}/epoch_predicted_top_k_std\"] = avg_epoch_predicted_top_k_std\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_mean\"] = avg_epoch_mean\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_log_variance\"] = avg_epoch_log_variance\n",
        "            group_avg_reward += np.sum(epoch_rewards)\n",
        "            total_valid_rewards += len(epoch_rewards)\n",
        "            print(f\"    {policy_name}: Avg Reward = {avg_epoch_reward:.4f}, Avg Predicted Top K = {avg_epoch_predicted_top_k:.2f}, Predicted Top K Std = {avg_epoch_predicted_top_k_std:.2f}\")\n",
        "\n",
        "        group_avg_reward = group_avg_reward / total_valid_rewards if total_valid_rewards > 0 else 0.0\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Best performing policy is {best_policy_name} with Avg Reward = {highest_avg_reward:.4f}\")\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Group Average Reward = {group_avg_reward:.4f}\")\n",
        "        epoch_metrics[\"epoch/best_policy\"] = best_policy_name\n",
        "        epoch_metrics[\"epoch/highest_avg_reward\"] = highest_avg_reward\n",
        "        epoch_metrics[\"epoch/group_average_reward\"] = group_avg_reward\n",
        "        wandb.log(epoch_metrics, step=epoch + 1)\n",
        "\n",
        "        # --- Establish Reference Policy ---\n",
        "        # If it's the first epoch, use the first policy as the reference\n",
        "        if epoch == 0:\n",
        "            reference_policy_state_dict = policy_group[0].state_dict()\n",
        "            print(\"  Epoch 1: Initialized reference policy from policy_group[0].\")\n",
        "        else:\n",
        "            # Find the best performing policy from the previous epoch's results\n",
        "            # This assumes policy_avg_rewards was populated in the previous epoch\n",
        "            # (which it is, at the end of the data collection phase)\n",
        "            best_policy_idx = int(best_policy_name.split('_')[1])\n",
        "            reference_policy_state_dict = policy_group[best_policy_idx].state_dict()\n",
        "            print(f\"  Epoch {epoch+1}: Updated reference policy from best performing policy ({best_policy_name}).\")\n",
        "\n",
        "        # --- Implement Policy Update Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Starting policy update...\")\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            optimizer = optimizers[policy_idx]\n",
        "\n",
        "            # Retrieve collected data (rewards, continuous samples, and questions)\n",
        "            policy_rewards_list = all_policy_rewards[policy_name]\n",
        "            policy_continuous_samples = all_policy_continuous_samples[policy_name] # List of tensors\n",
        "            policy_questions = all_policy_questions[policy_name] # List of strings\n",
        "\n",
        "            # Convert rewards to tensor (doesn't need grad for standard policy gradient)\n",
        "            policy_rewards = torch.tensor(policy_rewards_list, dtype=torch.float32)\n",
        "\n",
        "            # Recalculate advantages using the collected rewards\n",
        "            baseline = calculate_baseline(policy_rewards) # Calculate baseline from collected rewards\n",
        "            policy_advantages = policy_rewards - baseline # Calculate advantages\n",
        "\n",
        "            # Re-run forward pass on the stored questions to get log_probs that track gradients\n",
        "            if not policy_questions:\n",
        "                 print(f\"    {policy_name}: No questions available for update.\")\n",
        "                 wandb.log({f\"{policy_name}/policy_loss\": 0.0,}, step=epoch + 1)\n",
        "                 continue # Skip update if no questions\n",
        "\n",
        "            # Ensure policy is in training mode before forward pass for update\n",
        "            policy.train()\n",
        "            mean_output_update, log_variance_output_update = policy(policy_questions)\n",
        "\n",
        "            # Stack the collected continuous samples into a single tensor\n",
        "            if not policy_continuous_samples:\n",
        "                 print(f\"    {policy_name}: No continuous samples available for update.\")\n",
        "                 wandb.log({f\"{policy_name}/policy_loss\": 0.0,}, step=epoch + 1)\n",
        "                 continue # Skip update if no samples\n",
        "\n",
        "            policy_continuous_samples_tensor = torch.stack(policy_continuous_samples)\n",
        "\n",
        "            # Recalculate log probabilities using the fresh policy outputs and the stored continuous samples\n",
        "            policy_log_probs = calculate_log_prob(mean_output_update, log_variance_output_update, policy_continuous_samples_tensor)\n",
        "\n",
        "\n",
        "            # Filter out samples where original reward was 0 (likely due to errors in RAG execution)\n",
        "            # We use the original rewards to determine which samples were valid.\n",
        "            valid_indices = policy_rewards != 0\n",
        "            if torch.sum(valid_indices) > 0:\n",
        "                valid_log_probs = policy_log_probs[valid_indices]\n",
        "                valid_advantages = policy_advantages[valid_indices]\n",
        "\n",
        "                # Calculate the policy loss using the valid log probabilities and advantages\n",
        "                policy_loss = -torch.mean(valid_log_probs * valid_advantages)\n",
        "\n",
        "                # Perform optimizer.zero_grad() for the current policy's optimizer\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Call policy_loss.backward() to compute gradients\n",
        "                policy_loss.backward()\n",
        "\n",
        "                # Call optimizer.step() to update the current policy's parameters\n",
        "                optimizer.step()\n",
        "\n",
        "                # Log the policy loss for each policy after its update\n",
        "                wandb.log({\n",
        "                    f\"{policy_name}/policy_loss\": policy_loss.item(),\n",
        "                }, step=epoch + 1)\n",
        "\n",
        "            else:\n",
        "                wandb.log({\n",
        "                    f\"{policy_name}/policy_loss\": 0.0, # Log 0 loss if no valid samples for update\n",
        "                }, step=epoch + 1)\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Policy update completed.\")\n",
        "\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training skipped because CombinedFinancial10k index was not loaded.\")\n",
        "\n",
        "# Finish the Weights & Biases run\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOHQMGE8GtsqT090er6Y4ie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Reinforcement-Learning/blob/main/Agentic_RAG_RewardFunction_GRPO_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpEGNalHDvqV"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ul8zj9UhETD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "rRxaZjaZEXWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "D38Nn8M1Eey2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "ZiGL3yLfF-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex"
      ],
      "metadata": {
        "id": "fY04X58OGE9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "VWrslIC4GJw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ],
      "metadata": {
        "id": "f-ZbbEzxGSzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise a storage context and use that for both Vector Index and Summary Index for OECD\n",
        "#split the OECD document into multiple nodes\n",
        "oecd_nodes = Settings.node_parser.get_nodes_from_documents(docs_OECD_guidelines)\n",
        "#split the Form990 document into multiple nodes\n",
        "form990_nodes = Settings.node_parser.get_nodes_from_documents(docs_Form990_guidelines)\n",
        "\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "storage_context.docstore.add_documents(oecd_nodes)\n",
        "storage_context.docstore.add_documents(form990_nodes)\n",
        "# Setup Vector and Summary Index from Storage Context\n",
        "summary_index = SummaryIndex(oecd_nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(oecd_nodes, storage_context=storage_context)\n",
        "\n",
        "# Setup Indices.In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "OECD_index = get_index(\"OECDTPGuidelines\",f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\")\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\",f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\")"
      ],
      "metadata": {
        "id": "hutBG-82GyeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# Create the query engines\n",
        "OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "# Create tools for the query engines\n",
        "OECD_query_tool = QueryEngineTool(\n",
        "                      query_engine=OECD_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"OECD_QueryEngineTool_2022\",\n",
        "                          description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "Form990_query_tool = QueryEngineTool(\n",
        "                      query_engine=form990_guidelines_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"form990_2022\",\n",
        "                          description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "tools = [OECD_query_tool, Form990_query_tool]\n",
        "\n",
        "filing_engine = RouterQueryEngine(\n",
        "                      selector= LLMSingleSelector.from_defaults(),\n",
        "                      query_engine_tools=tools\n",
        "                      )"
      ],
      "metadata": {
        "id": "eF2CsXkzHBDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agentic Router RAG -\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "agent = OpenAIAgent.from_tools(tools=tools, verbose=True)\n",
        "# Uncomment and use the below call for interactive session\n",
        "#agent.chat_repl()\n",
        "response = agent.chat(\"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "hlIsonI9Hlx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent.openai import OpenAIAssistantAgent\n",
        "agent = OpenAIAssistantAgent.from_new(\n",
        "          name = \"OECD and Form990 Agent\",\n",
        "          instructions= \"You are an assistant that provides answers to questions on OECD and Form990. And make sure the answers are retreived form the OECD and Form990 pdf's only. No data from open Internet. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\",\n",
        "          tools=tools,\n",
        "          verbose=True,\n",
        "          run_retrieve_sleep_time=1.0\n",
        "        )\n",
        "response = agent.chat(\"What does Articles 9 and 25 of the OECD Model Tax Convention state?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "COLLlq4wJZZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"What does Articles 9 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Articles 25 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Allocation of Taxing Rights mean in OECD Model Tax Convention state?\",\n",
        "             \"How is Mutual Agreement Procedure(MAP) help in resolving disputes between countries when there's a conflict in interpreting the treaty?\",\n",
        "             \"As per OECD Model Tax Convention States what does Residence and Source Country mean?\"]\n",
        "ground_truth = [\"addresses corresponding adjustments in transfer pricing\",\n",
        "                \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\",\n",
        "                \"principles that determine how different jurisdictions can tax income generated by multinational enterprises (MNEs).\",\n",
        "                \"serves as a mechanism for tax administrations to consult and resolve disputes related to the interpretation and application of double tax conventions. It is particularly useful in situations where there is taxation not in accordance with the provisions of the Convention.\",\n",
        "                \"Resident country: The country where the taxpayer lives, Source country: The country where the income originates may also have taxing rights but often with limits.\"]"
      ],
      "metadata": {
        "id": "iomDElQZ-_s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "rs5zdr0s_Zkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage (assuming 'contexts' and 'ground_truth' are defined):\n",
        "# combined_context = \" \".join(contexts[0]) # Combine retrieved contexts\n",
        "# reward = cosine_similarity_reward(combined_context, ground_truth[0])\n",
        "# print(f\"Cosine Similarity Reward: {reward}\")"
      ],
      "metadata": {
        "id": "wtwCsHlESzC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers  = []\n",
        "contexts = []\n",
        "cosine_similarity_rewards = [] # List to store cosine similarity rewards\n",
        "\n",
        "\n",
        "# traversing each question and passing into the chain to get answer from the system\n",
        "# Define the retriever from the OECD index\n",
        "retriever = OECD_index.as_retriever()\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "    response = agent.chat(question)\n",
        "    answers.append(response.response) # Extract the string response\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_texts = [docs.node.text for docs in retrieved_docs]\n",
        "    contexts.append(context_texts)\n",
        "\n",
        "    # Calculate cosine similarity reward\n",
        "    # Combine retrieved contexts into a single string for similarity calculation\n",
        "    combined_context = \" \".join(context_texts)\n",
        "    cosine_similarity_reward_score = cosine_similarity_reward(combined_context, ground_truth[i])\n",
        "    cosine_similarity_rewards.append(cosine_similarity_reward_score)\n",
        "\n",
        "\n",
        "# Preparing the dataset\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"ground_truth\": ground_truth,\n",
        "    \"contexts\": contexts, # Add the contexts to the dataset\n",
        "    \"cosine_similarity_reward\": cosine_similarity_rewards, # Add the cosine similarity rewards\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset.to_pandas()"
      ],
      "metadata": {
        "id": "IMZHDx0B_dhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas --quiet\n",
        "import ragas"
      ],
      "metadata": {
        "id": "eHSrbhISAsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "q2XWstAEAxO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afbd2a"
      },
      "source": [
        "# Task\n",
        "Explain the selected code, discuss the next steps in RL after calculating rewards based on cosine similarity, explain how to update the policy directly in GRPO for the current situation, and define the policy network for this example, specifically referring to Group Relative Policy Optimization (GRPO) in the context of a RAG system where the reward is based on cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5a3b13"
      },
      "source": [
        "## Understand the role of the policy network\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of the RAG system the policy network will control (e.g., retrieval parameters, generation parameters, or both).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0389c9"
      },
      "source": [
        "## Choose a suitable neural network architecture\n",
        "\n",
        "### Subtask:\n",
        "Select a type of neural network (e.g., a feedforward network, recurrent network, or transformer-based model) that can process the input question and output the control signals for the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c336904d"
      },
      "source": [
        "**Reasoning**:\n",
        "To select a neural network architecture suitable for processing text input (the question) and outputting control signals for the RAG system, considering the sequential nature of text and the need to capture semantic meaning, a transformer-based model is a good choice due to its effectiveness in handling sequential data and complex relationships within text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b210d0"
      },
      "source": [
        "# Explanation of the chosen neural network architecture\n",
        "\n",
        "# For this task, where the input is a text question and the output needs to be control signals\n",
        "# for a RAG system (e.g., parameters for retrieval or generation), a transformer-based model\n",
        "# is a suitable choice.\n",
        "\n",
        "# Justification:\n",
        "# 1. Text Input: Transformers are specifically designed to process sequential data like text.\n",
        "# 2. Semantic Understanding: They excel at capturing long-range dependencies and semantic relationships\n",
        "#    within the input question, which is crucial for determining appropriate RAG control signals.\n",
        "# 3. Contextual Embeddings: Transformer models generate rich contextual embeddings of the input text,\n",
        "#    allowing the policy network to make informed decisions based on the nuances of the question.\n",
        "# 4. Adaptability: Transformer architectures can be adapted with output layers to produce numerical\n",
        "#    or categorical control signals as needed by the RAG system.\n",
        "\n",
        "# Examples of transformer-based models that could be adapted for this purpose include\n",
        "# models based on BERT, RoBERTa, or smaller, task-specific transformer variants.\n",
        "# The specific output layer would depend on the exact control signals being predicted (e.g.,\n",
        "# a dense layer for numerical parameters or a softmax layer for categorical choices).\n",
        "\n",
        "# This choice aligns with the task of learning a policy based on textual input to influence\n",
        "# the RAG system's behavior for better performance according to the defined reward."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2e884"
      },
      "source": [
        "## Define the input and output layers\n",
        "\n",
        "### Subtask:\n",
        "Define the format of the input (the question) and the output (the parameters or actions that influence the RAG system) for the chosen transformer-based policy network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c5cf8a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the input and output format of the chosen transformer-based policy network based on the previous subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a702d66a"
      },
      "source": [
        "# 1. Input Layer Format:\n",
        "# The input to the policy network will be the user's question, which is a string of text.\n",
        "# Before being fed into the transformer layers, this text will undergo standard NLP preprocessing steps:\n",
        "# - Tokenization: The text will be broken down into a sequence of tokens (words or sub-word units) using a tokenizer appropriate for the chosen transformer model (e.g., WordPiece for BERT, BPE for RoBERTa).\n",
        "# - Embedding: The sequence of tokens will be converted into a sequence of numerical embeddings. Transformer models typically use learned token embeddings, positional embeddings (to capture token order), and potentially segment embeddings. The input to the transformer layers will be a tensor of shape (batch_size, sequence_length, embedding_dim), where:\n",
        "#   - batch_size: The number of questions processed in parallel.\n",
        "#   - sequence_length: The maximum number of tokens in a question (padded or truncated).\n",
        "#   - embedding_dim: The dimensionality of the token embeddings.\n",
        "\n",
        "# 2. Output Layer(s) Format:\n",
        "# The output layer(s) of the policy network will produce control signals for the RAG system. Based on the understanding that the policy network controls retrieval parameters (like similarity_top_k) and potentially influences generation, the output could be structured as follows:\n",
        "# - For a numerical parameter like `similarity_top_k`: A single dense layer with one output neuron, potentially followed by an activation function (e.g., ReLU to ensure non-negativity) and possibly scaled to a reasonable range. The output would be a tensor of shape (batch_size, 1).\n",
        "# - For influencing generation (less direct control in this setup, but conceptually): This could be represented as a vector influencing attention mechanisms or providing context to the generation model. However, focusing on retrieval parameters as the primary policy output in this GRPO context is more straightforward.\n",
        "# - For simplicity and direct control over a key retrieval parameter, let's define the output as a single numerical value representing `similarity_top_k`. The output layer will be a dense layer with 1 output neuron.\n",
        "\n",
        "# Therefore, the output of the policy network will be a tensor of shape (batch_size, 1), representing the predicted value for `similarity_top_k` for each question in the batch.\n",
        "\n",
        "# 3. Interpretation and Usage of the Policy Network's Output:\n",
        "# The output of the policy network (the predicted `similarity_top_k` value) will be used to configure the retrieval step of the RAG system for the given question.\n",
        "# - During training: The predicted `similarity_top_k` will be used to perform retrieval. The retrieved context, along with the question, will then be passed to the generation model to produce an answer. This answer will be compared to the ground truth to calculate the cosine similarity reward. This reward will be used by the GRPO algorithm to update the policy network's weights, encouraging it to predict `similarity_top_k` values that lead to higher rewards.\n",
        "# - During inference: The policy network will predict `similarity_top_k` for a new question, and this value will be used directly in the retrieval process to gather context for generating the final answer.\n",
        "# The predicted numerical output for `similarity_top_k` might need to be post-processed (e.g., rounded to an integer, clipped to a valid range) before being used by the RAG system's retriever."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ece352"
      },
      "source": [
        "## Implement the policy network\n",
        "\n",
        "### Subtask:\n",
        "Implement the policy network using a deep learning framework. This involves defining the transformer layers and the output layer(s) based on the input and output formats defined in the previous steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20adbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy network using PyTorch, defining the transformer layers and the output layer as specified in previous steps.\n",
        "\n",
        "In essence, this network takes a question, processes it through a pre-trained transformer to understand its context, and then uses a simple linear layer to predict a non-negative numerical value intended to represent the optimal similarity_top_k for retrieving documents for that question.\n",
        "\n",
        "In the context of Reinforcement Learning (RL), a policy network is a neural network that learns to map states (in our case, the user's question) to actions (the parameters or decisions that control the RAG system).\n",
        "\n",
        "**State:** The input is the user's question. The policy network processes this question to understand its meaning and context.\n",
        "\n",
        "**Action:** The output of the policy network is a value (or values) that influences how the RAG system operates. In the code we just discussed, the policy network's action space was initially simplified to predicting a single value: similarity_top_k, which determines how many relevant documents are retrieved. In the modified code, it predicts parameters for a distribution from which similarity_top_k is sampled.\n",
        "\n",
        "The goal of training the policy network using an algorithm like GRPO is to adjust its internal parameters (the weights and biases of the neural network) so that, when presented with a question, it predicts/samples actions (like a specific similarity_top_k) that lead to higher rewards (in our case, higher cosine similarity between the generated answer and the ground truth).\n",
        "\n",
        "So, the policy network's role is to learn the optimal strategy for configuring the RAG system based on the input question to maximize the desired outcome (answer quality, measured by the reward).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04ca0af"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The policy network is designed to control the `similarity_top_k` parameter in the retrieval step of the RAG system.\n",
        "*   A transformer-based model (like BERT) is chosen as the architecture for the policy network due to its effectiveness in processing text input (the user question).\n",
        "*   The input to the policy network is the tokenized and embedded user question, and the output is a single numerical value representing the predicted `similarity_top_k`.\n",
        "*   The implemented policy network uses a pre-trained BERT model and a linear output layer with a ReLU activation to predict a non-negative `similarity_top_k`.\n",
        "*   The policy network's output is integrated into the RAG query process by dynamically setting the `similarity_top_k` of the retriever used by the query engine.\n",
        "*   The training process involves iteratively: predicting `similarity_top_k` using the policy, executing the RAG system, calculating the cosine similarity reward between the generated answer and the ground truth, and updating the policy network's weights using a policy gradient method (aligned with GRPO principles) to maximize the expected reward.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The predicted `similarity_top_k` value should be carefully post-processed (e.g., rounded, clipped to a valid range based on the document index size) before being used in the retriever to ensure it is a valid and effective parameter.\n",
        "*   Implementing the full GRPO algorithm would involve more complex components than a basic policy gradient, potentially including maintaining a group of policies, comparing their performance, and using trust region methods to constrain updates. A practical next step is to implement a policy gradient training loop with baseline subtraction and potentially explore using a continuous action space policy outputting the parameters of a distribution (like mean and variance) for `similarity_top_k`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ff9198"
      },
      "source": [
        "# Task\n",
        "**Implement the policy update rule for the GRPO algorithm using cosine similarity as the reward signal to adjust the policy network's parameters in the provided notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a4c096"
      },
      "source": [
        "## Modify policy network output\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `RAGPolicyNetwork` to output parameters for a distribution over `similarity_top_k`, such as the mean and log-variance of a Gaussian distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fb1c21"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the RAGPolicyNetwork class to output parameters for a Gaussian distribution (mean and log-variance) over `similarity_top_k`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515fe81f"
      },
      "source": [
        "## Implement action sampling and log probability calculation\n",
        "\n",
        "### Subtask:\n",
        "Implement functions to sample `similarity_top_k` from the Gaussian distribution predicted by the policy network and calculate the log probability of the sampled action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71623969"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement functions to sample the action (similarity_top_k) from the predicted Gaussian distribution and calculate the log probability of the sampled action, using the predicted mean and log-variance from the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60f1e206"
      },
      "source": [
        "## Implement baseline calculation\n",
        "\n",
        "### Subtask:\n",
        "Create a function to calculate a baseline for the rewards, such as the average reward in a batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160b14ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to calculate the mean of a list or tensor of rewards to be used as a baseline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422375d8"
      },
      "source": [
        "## Set up training loop\n",
        "\n",
        "### Subtask:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8558acf5"
      },
      "source": [
        "**Reasoning**:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57939ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Fix the `IndexError` by ensuring `sampled_k_processed[i]` is treated correctly as a tensor within the loop, likely by accessing its value using `.item()` after indexing, as suggested by the error message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8479c2f2"
      },
      "source": [
        "## Implement policy update\n",
        "\n",
        "### Subtask:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using an optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0307182"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03bb1ac3"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "After training, evaluate the performance of the policy-controlled RAG system and refine the implementation or hyperparameters as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81edce18"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the policy-controlled RAG system after the training loop. This involves setting the policy network to evaluation mode, using a dataset (can be the same as training for demonstration), iterating through questions, predicting/sampling `similarity_top_k`, executing the RAG query, calculating the cosine similarity reward, and finally reporting the average reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f3175ba"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `policy_controlled_rag_query` function was expecting a tensor output from the policy network, but the modified policy network now returns a tuple of tensors (mean and log-variance). The `policy_controlled_rag_query` function needs to be updated to handle this new output format, sample an action (similarity_top_k) from the predicted distribution, and then use that sampled action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3dd62f0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `RAGPolicyNetwork` was successfully modified to output parameters (mean and log-variance) for a Gaussian distribution over `similarity_top_k`.\n",
        "*   Functions were successfully implemented to sample `similarity_top_k` from the predicted Gaussian distribution and calculate the log probability of the sampled action, including handling the conversion from a continuous sample to a discrete, positive integer.\n",
        "*   A function to calculate the mean reward as a baseline was successfully implemented and tested.\n",
        "*   A training loop structure was successfully set up, integrating the policy network forward pass, action sampling, a placeholder for RAG execution and reward calculation (using a dummy reward for demonstration), baseline calculation, advantage computation, policy loss calculation, backpropagation, and optimizer steps.\n",
        "*   The core policy update steps (`backward()` and `optimizer.step()`) were confirmed to be correctly integrated and executed within the training loop.\n",
        "*   An evaluation process was implemented to test the trained policy, showing that the policy network predicted `similarity_top_k` values (10 and 11 in the example) and resulted in an average cosine similarity reward of 0.3299 on the evaluation set.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current low average reward (0.3299) suggests that the policy is not yet effectively learning to choose `similarity_top_k` values that maximize cosine similarity. Further refinement of the reward function, action space handling, or policy network architecture is needed.\n",
        "*   Implementing the actual RAG execution and cosine similarity reward calculation within the training loop is the crucial next step to train the policy network on real rewards rather than a dummy reward. This will require integrating the RAG system (using `OECD_index` and `agent`) into the training loop's batch processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287a48b1"
      },
      "source": [
        "# Task\n",
        "Implement observability metrics for the training process of a policy network using Weights and Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0da520a7"
      },
      "source": [
        "## Identify key metrics\n",
        "\n",
        "### Subtask:\n",
        "Determine which metrics are most important to track for monitoring the training process of the policy network (e.g., epoch number, batch number, average policy loss per batch, average reward per batch, average predicted similarity_top_k per batch, average advantage per batch).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e8b7ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify and list the key metrics for monitoring the training process of the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459dfae1"
      },
      "source": [
        "## Integrate weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Install the `wandb` library and initialize a Weights & Biases run at the beginning of the training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb8ada8"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the wandb library and initialize a Weights & Biases run at the beginning of the training script as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce3954cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the variables `questions` and `ground_truth` were not defined in the current code cell. These variables were defined in a previous cell and need to be accessible. The code block should be re-executed including the definitions of `questions` and `ground_truth`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60b5cd9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `policy_network` variable was not defined. This variable was instantiated in a previous cell and needs to be accessible in the current cell for the optimizer to be created and for the training loop to run. The code block should be re-executed including the definition and instantiation of the `RAGPolicyNetwork` class and the `policy_network` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39a23eaa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the function `sample_action_and_continuous` was not defined in the current code cell. This function, along with `calculate_baseline` and `calculate_log_prob`, are necessary for the training loop to execute. These functions need to be included in the current code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ca260e"
      },
      "source": [
        "## Log hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Log the training hyperparameters (e.g., learning rate, batch size, number of epochs) to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22da477d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a dictionary containing the training hyperparameters and log it to the initialized Weights & Biases run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59aecdad"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `wandb.init()` was not called before `wandb.config.update()` in the current execution environment. Although `wandb.init()` was in a previous cell, each cell is executed independently. To fix this, I need to call `wandb.init()` again in the same cell before `wandb.config.update()`. I will also redefine the necessary variables and functions to ensure they are available in this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de2b5e9"
      },
      "source": [
        "## Integrate metric calculation into the training loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to calculate the chosen metrics for each batch and/or epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9543160b"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to calculate the selected metrics for each batch and epoch, including average policy loss, average reward, average predicted similarity_top_k, average advantage, average mean, and average log-variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b06792f"
      },
      "source": [
        "## Log metrics to weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop. This will typically involve using `wandb.log()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48c0ca5"
      },
      "source": [
        "**Reasoning**:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop as requested by the subtask. This involves using `wandb.log()` for both batch and epoch metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbaad26"
      },
      "source": [
        "## Visualize and analyze metrics in weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Use the Weights & Biases dashboard to visualize the logged metrics, monitor training progress, and identify potential issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b145d0fd"
      },
      "source": [
        "## Refine logging and metrics\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked or the logging frequency for better insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d72db7"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked and their logging frequency in the training code. The current metrics are informative, but batch-level metrics can be noisy. Epoch-level metrics provide a smoother view of overall progress. We will keep logging both but ensure epoch metrics are clearly distinguished. We can also consider adding the standard deviation of `similarity_top_k` predictions at the epoch level to see how the policy's uncertainty evolves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ],
      "metadata": {
        "id": "vouyCtSmrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop includes the actual RAG execution using the OECD_index and the cosine_similarity_reward function to calculate the reward based on the generated answer and the ground truth for each question in the batch. This means the policy network is being trained using real rewards derived from the RAG system's performance."
      ],
      "metadata": {
        "id": "y0UM0l3tbSZ5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4402a427"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # Explicitly import torch.nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torch.distributions import Normal\n",
        "import wandb\n",
        "import os # Import os to check for existing index\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, StorageContext, load_index_from_storage, Settings # Import necessary LlamaIndex components\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from google.colab import userdata\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoModel, AutoTokenizer # Import AutoModel and AutoTokenizer explicitly\n",
        "\n",
        "\n",
        "# Redefine necessary variables and functions from previous cells to ensure scope\n",
        "\n",
        "# Assuming OPENAI_API_KEY is already set as an environment variable in a previous cell\n",
        "# os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Setup OpenAI Model and Embeddings - Ensure these are set within this cell's execution\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024\n",
        "print(\"LlamaIndex Settings configured.\")\n",
        "\n",
        "# Assuming Google Drive is mounted at /content/drive and data_dir is defined\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "\n",
        "# Redefine get_index function if needed (assuming index is persisted)\n",
        "# In this case, we will just load the index directly assuming it exists from previous runs\n",
        "# If you haven't run the cells to create and persist the index, you would need to do that first.\n",
        "\n",
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "# Assuming the index for OECD is already created and persisted in a previous run\n",
        "try:\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}OECDTPGuidelines/\")\n",
        "    OECD_index = load_index_from_storage(storage_context)\n",
        "    print(\"Loaded OECD index from storage.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"OECD index not found at {PERSIST_INDEX_DIR}OECDTPGuidelines/. Please run the cells to create and persist the index first.\")\n",
        "    # Handle the error, e.g., exit or create the index\n",
        "    OECD_index = None # Set to None if not loaded\n",
        "\n",
        "# Redefine cosine_similarity_reward function\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Redefine sample_action_and_continuous function\n",
        "def sample_action_and_continuous(mean, log_variance):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    continuous_sample = distribution.sample()\n",
        "    processed_action = torch.max(torch.tensor(1.0), torch.round(torch.abs(continuous_sample)))\n",
        "    return processed_action, continuous_sample\n",
        "\n",
        "# Redefine calculate_baseline function\n",
        "def calculate_baseline(rewards):\n",
        "    if isinstance(rewards, list):\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    if rewards.numel() == 0:\n",
        "        return 0.0\n",
        "    return torch.mean(rewards)\n",
        "\n",
        "def calculate_log_prob(mean, log_variance, action):\n",
        "    std_dev = torch.exp(0.5 * log_variance)\n",
        "    distribution = Normal(mean, std_dev)\n",
        "    log_prob = distribution.log_prob(action)\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "# Redefine questions and ground truth\n",
        "questions = [\"What does Articles 9 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Articles 25 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Allocation of Taxing Rights mean in OECD Model Tax Convention state?\",\n",
        "             \"How is Mutual Agreement Procedure(MAP) help in resolving disputes between countries when there's a conflict in interpreting the treaty?\",\n",
        "             \"As per OECD Model Tax Convention States what does Residence and Source Country mean?\"]\n",
        "ground_truth = [\"addresses corresponding adjustments in transfer pricing\",\n",
        "                \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\",\n",
        "                \"principles that determine how different jurisdictions can tax income generated by multinational enterprises (MNEs).\",\n",
        "                \"serves as a mechanism for tax administrations to consult and resolve disputes related to the interpretation and application of double tax conventions. It is particularly useful in situations where there is taxation not in accordance with the provisions of the Convention.\",\n",
        "                \"Resident country: The country where the taxpayer lives, Source country: The country where the income originates may also have taxing rights but often with limits.\"]\n",
        "\n",
        "# Redefine RAGPolicyNetwork class\n",
        "class RAGPolicyNetwork(nn.Module):\n",
        "    def __init__(self, transformer_model_name=\"bert-base-uncased\", output_dim=2):\n",
        "        super(RAGPolicyNetwork, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
        "        transformer_output_dim = self.transformer.config.hidden_size\n",
        "        self.output_layer = nn.Linear(transformer_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        encoded_input = self.tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
        "        outputs = self.transformer(**encoded_input)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        mean_and_log_variance = self.output_layer(pooled_output)\n",
        "        mean = mean_and_log_variance[:, 0]\n",
        "        log_variance = mean_and_log_variance[:, 1]\n",
        "        return mean, log_variance\n",
        "\n",
        "# Instantiate the policy network again\n",
        "# Ensure this is done after defining the class\n",
        "policy_network = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "# Redefine Dataset and DataLoader\n",
        "class RAGDataset(Dataset):\n",
        "    def __init__(self, questions, ground_truth):\n",
        "        self.questions = questions\n",
        "        self.ground_truth = ground_truth\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.questions[idx], self.ground_truth[idx]\n",
        "\n",
        "rag_dataset = RAGDataset(questions, ground_truth)\n",
        "BATCH_SIZE = 8\n",
        "train_dataloader = DataLoader(rag_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# Initialize a Weights & Biases run\n",
        "# Use reinit=True to allow re-initialization in a notebook environment\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()\n",
        "wandb.init(project=\"rag-policy-training\", name=\"grpo-cosine-similarity-refined-metrics\", reinit=True)\n",
        "\n",
        "# Define and log hyperparameters\n",
        "config = {\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"transformer_model\": \"bert-base-uncased\",\n",
        "    \"output_dim\": 2\n",
        "}\n",
        "wandb.config.update(config)\n",
        "\n",
        "print(\"Training hyperparameters logged to Weights & Biases config.\")\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"Starting policy network training...\")\n",
        "\n",
        "# Calculate total steps for logging\n",
        "total_steps = NUM_EPOCHS * len(train_dataloader) # Calculates the total number of batches that will be processed across all epochs, useful for a global step count in logging.\n",
        "global_step = 0 # Initializes a counter for the global step, incremented after processing each batch.\n",
        "\n",
        "# Check if OECD_index was loaded successfully before starting training\n",
        "if OECD_index is not None: # Ensures that the training process only starts if the necessary OECD index was successfully loaded from storage.\n",
        "    for epoch in range(NUM_EPOCHS): # Starts the outer loop which iterates over the defined number of training epochs.\n",
        "        policy_network.train() # Sets the policy network module to training mode. This affects behaviors like dropout and batch normalization.\n",
        "        total_epoch_loss = 0 # Initializes a variable to accumulate the policy loss across all batches in the current epoch.\n",
        "        total_epoch_reward = 0 # Initializes a variable to accumulate the sum of rewards across all batches in the current epoch.\n",
        "        total_epoch_predicted_top_k = 0 # Initializes a variable to accumulate the sum of predicted similarity_top_k values across all batches in the current epoch.\n",
        "        total_epoch_advantage = 0 # Initializes a variable to accumulate the sum of advantages across all batches in the current epoch.\n",
        "        total_epoch_mean = 0 # Initializes a variable to accumulate the sum of predicted means across all batches in the current epoch.\n",
        "        total_epoch_log_variance = 0 # Initializes a variable to accumulate the sum of predicted log variances across all batches in the current epoch.\n",
        "        epoch_predicted_top_ks = [] # Initializes a list to store individual predicted similarity_top_k values for calculating the standard deviation at the end of the epoch.\n",
        "        num_batches = 0 # Initializes a counter for the number of batches processed in the current epoch.\n",
        "\n",
        "        for batch_idx, (batch_questions, batch_ground_truth) in enumerate(train_dataloader): # Starts the inner loop, iterating through batches of data from the training DataLoader. `batch_idx` is the index of the current batch.\n",
        "            global_step += 1 # Increments the global step counter after processing each batch.\n",
        "\n",
        "            optimizer.zero_grad() # Clears the gradients of all optimized tensors. This is important before computing gradients for the current batch.\n",
        "\n",
        "            # a. Perform a forward pass through the policy network\n",
        "            mean_output, log_variance_output = policy_network(list(batch_questions)) # Passes the batch of questions (converted to a list) through the policy network's forward method to get the predicted mean and log-variance for the action distribution.\n",
        "\n",
        "            batch_sampled_k_processed = [] # Initializes a list to store the post-processed (integer, positive) sampled similarity_top_k values for the current batch.\n",
        "            batch_sampled_k_continuous = [] # Initializes a list to store the original continuous sampled values from the Gaussian distribution for the current batch.\n",
        "            batch_rewards = [] # Initializes a list to store the calculated rewards for each question in the current batch.\n",
        "\n",
        "            for i in range(len(batch_questions)): # Starts a loop to process each question individually within the current batch.\n",
        "                # b. Use the sample_action_and_continuous function to sample similarity_top_k actions\n",
        "                sampled_k_processed_item, sampled_k_continuous_item = sample_action_and_continuous(mean_output[i], log_variance_output[i]) # Calls the helper function to sample an action (similarity_top_k) from the predicted distribution for the i-th question, getting both the processed integer value and the original continuous sample.\n",
        "\n",
        "                batch_sampled_k_processed.append(sampled_k_processed_item) # Appends the processed (integer) sampled action to the list.\n",
        "                batch_sampled_k_continuous.append(sampled_k_continuous_item) # Appends the continuous sampled action to the list.\n",
        "                epoch_predicted_top_ks.append(sampled_k_processed_item.item()) # Appends the item value of the processed sampled action to the epoch list for standard deviation calculation.\n",
        "\n",
        "                # --- Integrate Actual RAG Execution and Reward Calculation ---\n",
        "                question = batch_questions[i] # Gets the current question string.\n",
        "                ground_truth_answer = batch_ground_truth[i] # Gets the corresponding ground truth answer string.\n",
        "                predicted_top_k_int = int(sampled_k_processed_item.item()) # Converts the sampled similarity_top_k item to an integer for use in the RAG system.\n",
        "\n",
        "                try: # Starts a try block to handle potential errors during RAG execution or reward calculation.\n",
        "                    # Execute the RAG system using the sampled similarity_top_k\n",
        "                    # Create a temporary query engine with the policy-controlled retriever\n",
        "                    policy_controlled_engine = OECD_index.as_query_engine(similarity_top_k=predicted_top_k_int) # Creates a query engine instance from the OECD index, configured with the policy-sampled similarity_top_k.\n",
        "                    generated_answer = policy_controlled_engine.query(question).response # Executes a query on the policy-controlled engine with the current question and extracts the generated answer text.\n",
        "\n",
        "                    # Calculate the cosine similarity reward\n",
        "                    reward = cosine_similarity_reward(generated_answer, ground_truth_answer) # Calculates the cosine similarity reward between the generated answer and the ground truth.\n",
        "                    batch_rewards.append(reward) # Appends the calculated reward to the list of batch rewards.\n",
        "\n",
        "                except Exception as e: # Catches any exception that occurs within the try block.\n",
        "                    print(f\"Error during RAG execution or reward calculation for question '{question}': {e}\") # Prints an error message including the question and the exception details.\n",
        "                    # Append a placeholder reward in case of error\n",
        "                    batch_rewards.append(0.0) # Appends a reward of 0.0 to the batch rewards list to handle errors gracefully and prevent the training from crashing.\n",
        "                # --- End Actual RAG Execution and Reward Calculation ---\n",
        "\n",
        "\n",
        "            batch_sampled_k_continuous_tensor = torch.stack(batch_sampled_k_continuous) # Stacks the list of continuous sampled actions into a single tensor.\n",
        "            batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32) # Converts the list of batch rewards into a PyTorch tensor with float32 data type.\n",
        "\n",
        "            # e. Calculate the baseline reward for the batch\n",
        "            baseline = calculate_baseline(batch_rewards_tensor) # Calculates the baseline (average) reward for the current batch.\n",
        "\n",
        "            # f. Calculate the advantage for each sample\n",
        "            # a low reward (especially lower than the baseline, resulting in a negative advantage)\n",
        "            advantage = batch_rewards_tensor - baseline # Calculates the advantage for each sample by subtracting the baseline reward from the individual reward.\n",
        "\n",
        "            # g. Calculate the log probability of the original continuous sampled actions\n",
        "            # the policy loss calculation involves multiplying the log probability of the action taken by a negative advantage\n",
        "            log_probs = calculate_log_prob(mean_output, log_variance_output, batch_sampled_k_continuous_tensor) # Calculates the log probability of the original continuous sampled actions under the distribution predicted by the policy network.\n",
        "\n",
        "            # Since the log probability of the action taken is usually negative (as probabilities are between 0 and 1, and log(probability) is negative), and the advantage is negative, the product becomes positive.\n",
        "            # h. Compute the policy loss\n",
        "            # because the policy gradient loss is defined as the negative of this product (-torch.mean(log_probs * advantage)), the resulting loss contribution for that sample will be negative.\n",
        "            policy_loss = -torch.mean(log_probs * advantage) # Computes the policy loss using the policy gradient formula: the negative mean of the element-wise product of log probabilities and advantages.\n",
        "\n",
        "            # i. Perform a backward pass to compute gradients\n",
        "            policy_loss.backward() # Computes the gradients of the policy loss with respect to the policy network's parameters using backpropagation.\n",
        "\n",
        "            # j. Update the policy network's weights\n",
        "            optimizer.step() # Updates the policy network's parameters using the optimizer based on the computed gradients.\n",
        "\n",
        "            # Calculate batch metrics\n",
        "            batch_policy_loss = policy_loss.item() # Gets the scalar value of the batch policy loss.\n",
        "            batch_average_reward = torch.mean(batch_rewards_tensor).item() # Calculates the average reward for the batch and gets its scalar value.\n",
        "            batch_average_predicted_top_k = torch.mean(torch.stack(batch_sampled_k_processed).float()).item() # Calculates the average processed predicted similarity_top_k for the batch and gets its scalar value.\n",
        "            batch_average_advantage = torch.mean(advantage).item() # Calculates the average advantage for the batch and gets its scalar value.\n",
        "            batch_average_mean = torch.mean(mean_output).item() # Calculates the average predicted mean for the batch and gets its scalar value.\n",
        "            batch_average_log_variance = torch.mean(log_variance_output).item() # Calculates the average predicted log variance for the batch and gets its scalar value.\n",
        "\n",
        "            # Accumulate metrics for epoch averages\n",
        "            total_epoch_loss += batch_policy_loss # Adds the batch loss to the total epoch loss.\n",
        "            total_epoch_reward += torch.sum(batch_rewards_tensor).item() # Adds the sum of batch rewards to the total epoch reward.\n",
        "            total_epoch_predicted_top_k += torch.sum(torch.stack(batch_sampled_k_processed).float()).item() # Adds the sum of batch predicted top_k to the total epoch predicted top_k.\n",
        "            total_epoch_advantage += torch.sum(advantage).item() # Adds the sum of batch advantages to the total epoch advantage.\n",
        "            total_epoch_mean += torch.sum(mean_output).item() # Adds the sum of batch means to the total epoch mean.\n",
        "            total_epoch_log_variance += torch.sum(log_variance_output).item() # Adds the sum of batch log variances to the total epoch log variance.\n",
        "\n",
        "            num_batches += 1 # Increments the batch counter for the current epoch.\n",
        "\n",
        "            # Log batch metrics to Weights & Biases\n",
        "            wandb.log({ # Logs the calculated batch-level metrics to Weights & Biases.\n",
        "                \"batch/policy_loss\": batch_policy_loss,\n",
        "                \"batch/average_reward\": batch_average_reward,\n",
        "                \"batch/average_predicted_top_k\": batch_average_predicted_top_k,\n",
        "                \"batch/average_advantage\": batch_average_advantage,\n",
        "                \"batch/average_mean\": batch_average_mean,\n",
        "                \"batch/average_log_variance\": batch_average_log_variance,\n",
        "            }, step=global_step) # Uses the global step for logging.\n",
        "\n",
        "\n",
        "        # Calculate epoch metrics after the batch loop\n",
        "        avg_epoch_loss = total_epoch_loss / num_batches if num_batches > 0 else 0 # Calculates the average epoch loss.\n",
        "        avg_epoch_reward = total_epoch_reward / len(rag_dataset) if len(rag_dataset) > 0 else 0 # Calculates the average epoch reward per sample.\n",
        "        avg_epoch_predicted_top_k = total_epoch_predicted_top_k / len(rag_dataset) if len(rag_dataset) > 0 else 0 # Calculates the average predicted similarity_top_k per sample for the epoch.\n",
        "        avg_epoch_advantage = total_epoch_advantage / len(rag_dataset) if len(rag_dataset) > 0 else 0 # Calculates the average advantage per sample for the epoch.\n",
        "        avg_epoch_mean = total_epoch_mean / len(rag_dataset) if len(rag_dataset) > 0 else 0 # Calculates the average predicted mean per sample for the epoch.\n",
        "        avg_epoch_log_variance = total_epoch_log_variance / len(rag_dataset) if len(rag_dataset) > 0 else 0 # Calculates the average predicted log variance per sample for the epoch.\n",
        "        epoch_predicted_top_k_std = np.std(epoch_predicted_top_ks) if epoch_predicted_top_ks else 0.0 # Calculates the standard deviation of the predicted similarity_top_k values across the epoch.\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Avg Loss: {avg_epoch_loss:.4f}, Avg Reward: {avg_epoch_reward:.4f}, Avg Predicted Top K: {avg_epoch_predicted_top_k:.2f}, Predicted Top K Std: {epoch_predicted_top_k_std:.2f}\") # Prints the epoch summary metrics to the console.\n",
        "\n",
        "        # Log epoch metrics to Weights & Biases\n",
        "        wandb.log({ # Logs the calculated epoch-level metrics to Weights & Biases.\n",
        "            \"epoch/average_loss\": avg_epoch_loss,\n",
        "            \"epoch/average_reward\": avg_epoch_reward,\n",
        "            \"epoch/average_predicted_top_k\": avg_epoch_predicted_top_k,\n",
        "            \"epoch/average_advantage\": avg_epoch_advantage,\n",
        "            \"epoch/average_mean\": avg_epoch_mean,\n",
        "            \"epoch/average_log_variance\": avg_epoch_log_variance,\n",
        "            \"epoch/predicted_top_k_std\": epoch_predicted_top_k_std # Log standard deviation\n",
        "        }, step=epoch + 1) # Uses the epoch number for logging.\n",
        "\n",
        "    print(\"Training finished.\") # Prints a message indicating the training is complete.\n",
        "\n",
        "else:\n",
        "    print(\"Training skipped because OECD index was not loaded.\") # Prints a message if training was skipped due to the index not loading.\n",
        "\n",
        "# Finish the Weights & Biases run\n",
        "if wandb.run is not None: # Checks if a Weights & Biases run is currently active.\n",
        "    wandb.finish() # Finishes the Weights & Biases run, ensuring all data is synced."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de918f7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The training process successfully logged key metrics at both the batch and epoch levels to Weights & Biases, including policy loss, average reward, average predicted `similarity_top_k`, average advantage, average mean, and average log-variance of the predicted distribution.\n",
        "*   Hyperparameters such as learning rate (1e-4), batch size (8), and number of epochs (100) were successfully logged to the Weights & Biases config.\n",
        "*   The training loop executed for 100 epochs, with console output confirming the progress and epoch-average loss and reward.\n",
        "*   A new epoch-level metric, the standard deviation of the predicted `similarity_top_k`, was successfully added and logged to provide insight into the variability of the policy's actions.\n",
        "\n",
        "### Completed Steps\n",
        "\n",
        "*   Visualized the logged metrics in the Weights & Biases dashboard to analyze trends, identify correlations between metrics (e.g., reward and predicted top\\_k), and diagnose potential training issues such as instability or convergence problems.\n",
        "*   Implement the actual RAG system reward calculation to replace the dummy reward function, allowing the policy to learn based on real retrieval performance.\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Reinforcement-Learning/blob/main/Agentic_RewardFunction_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpEGNalHDvqV"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q\n",
        "!pip install jedi\n",
        "!pip install --upgrade openai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=\"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document\"\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ],
      "metadata": {
        "id": "Ukh_iMPCJo09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ul8zj9UhETD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "rRxaZjaZEXWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "D38Nn8M1Eey2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # Explicitly import torch.nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torch.distributions import Normal\n",
        "import os # Import os to check for existing index\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, StorageContext, load_index_from_storage, Settings # Import necessary LlamaIndex components\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ZiGL3yLfF-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ],
      "metadata": {
        "id": "f-ZbbEzxGSzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "jt-LRHKGw6j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise a storage context and use that for both Vector Index and Summary Index for OECD\n",
        "#split the OECD document into multiple nodes\n",
        "oecd_nodes = Settings.node_parser.get_nodes_from_documents(docs_OECD_guidelines)\n",
        "#split the Form990 document into multiple nodes\n",
        "form990_nodes = Settings.node_parser.get_nodes_from_documents(docs_Form990_guidelines)\n",
        "\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "storage_context.docstore.add_documents(oecd_nodes)\n",
        "storage_context.docstore.add_documents(form990_nodes)\n",
        "# Setup Vector and Summary Index from Storage Context\n",
        "summary_index = SummaryIndex(oecd_nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(oecd_nodes, storage_context=storage_context)\n",
        "\n",
        "# Setup Indices.In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "OECD_index = get_index(\"OECDTPGuidelines\",f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\")\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\",f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\")\n",
        ""
      ],
      "metadata": {
        "id": "OlqqzfSYwyCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ],
      "metadata": {
        "id": "vouyCtSmrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c6adb5a"
      },
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "import json # Import the json module for schema definition\n",
        "\n",
        "# Redefine or ensure tools are available from previous successful cells\n",
        "# Assuming OECD_query_tool and Form990_query_tool are defined here or in a prior cell\n",
        "# Example definition if not already available:\n",
        "# OECD_engine = OECD_index.as_query_engine(similarity_top_k=3) # Assuming OECD_index is loaded\n",
        "# form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3) # Assuming form990_guidelines_index is loaded\n",
        "# OECD_query_tool = QueryEngineTool(\n",
        "#                       query_engine=OECD_engine,\n",
        "#                       metadata=ToolMetadata(\n",
        "#                           name=\"OECD_QueryEngineTool_2022\",\n",
        "#                           description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "#                       )\n",
        "#                     )\n",
        "# Form990_query_tool = QueryEngineTool(\n",
        "#                       query_engine=form990_guidelines_engine,\n",
        "#                       metadata=ToolMetadata(\n",
        "#                           name=\"form990_2022\",\n",
        "#                           description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "#                       )\n",
        "#                     )\n",
        "# tools = [OECD_query_tool, Form990_query_tool] # Ensure 'tools' list is defined\n",
        "\n",
        "# 1. Define a function openai_tool_definition\n",
        "def openai_tool_definition(query_engine_tool: QueryEngineTool) -> dict:\n",
        "    \"\"\"\n",
        "    Converts a LlamaIndex QueryEngineTool to the OpenAI Chat Completions API\n",
        "    tool format.\n",
        "\n",
        "    Args:\n",
        "        query_engine_tool: The QueryEngineTool instance.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the tool definition in OpenAI's format.\n",
        "    \"\"\"\n",
        "    # 4. Define the parameters schema\n",
        "    parameters_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"input\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The query string to pass to the tool.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"input\"],\n",
        "    }\n",
        "\n",
        "    # 2. Construct the dictionary for the tool definition\n",
        "    tool_definition = {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            # 3. Extract name and description from metadata\n",
        "            \"name\": query_engine_tool.metadata.name,\n",
        "            \"description\": query_engine_tool.metadata.description,\n",
        "            \"parameters\": parameters_schema,\n",
        "        },\n",
        "    }\n",
        "    return tool_definition\n",
        "\n",
        "# Create query engines\n",
        "OECD_engine = OECD_index.as_query_engine(similarity_top_k=3) # Assuming OECD_index is loaded\n",
        "form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3) # Assuming form990_guidelines_index is loaded\n",
        "\n",
        "# Create QueryEngineTool instances\n",
        "OECD_query_tool = QueryEngineTool(\n",
        "                      query_engine=OECD_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"OECD_QueryEngineTool_2022\",\n",
        "                          description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "Form990_query_tool = QueryEngineTool(\n",
        "                      query_engine=form990_guidelines_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"form990_2022\",\n",
        "                          description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "                      )\n",
        "                    )\n",
        "# Create the tools list\n",
        "tools = [OECD_query_tool, Form990_query_tool]\n",
        "\n",
        "\n",
        "# 5. Apply this function to existing tools\n",
        "# Ensure 'tools' list is available from previous cells\n",
        "# If not, you'd need to define OECD_query_tool and Form990_query_tool here\n",
        "# Assuming 'tools' is defined and contains OECD_query_tool and Form990_query_tool\n",
        "\n",
        "if 'tools' in globals() and isinstance(tools, list) and len(tools) > 0:\n",
        "    openai_tools = [openai_tool_definition(tool) for tool in tools]\n",
        "    print(\"OpenAI tool definitions created successfully:\")\n",
        "    # Print the created tool definitions for verification\n",
        "    print(json.dumps(openai_tools, indent=2))\n",
        "else:\n",
        "    print(\"Error: 'tools' list not found or is empty. Cannot create OpenAI tool definitions.\")\n",
        "    openai_tools = [] # Initialize as empty if tools are not available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bacfe15"
      },
      "source": [
        "## Implement chat completions logic\n",
        "\n",
        "### Subtask:\n",
        "Create a function or class that uses `openai.ChatCompletion.create` (or the equivalent using the `openai` library) to send messages to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84c459f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `chat_with_tools` that uses `openai.ChatCompletion.create` (or the equivalent) to send messages and tools to the model and handle the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4893a3f9"
      },
      "source": [
        "import openai\n",
        "import os # Import os to access environment variables\n",
        "\n",
        "# Ensure the OpenAI API key is set\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
        "    # In a real application, you would handle this more robustly\n",
        "    # For this example, we'll proceed but the API call will fail.\n",
        "\n",
        "\n",
        "# 1. Define a Python function, for example chat_with_tools\n",
        "def chat_with_tools(messages: list[dict], openai_tools: list[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.7) -> dict:\n",
        "    \"\"\"\n",
        "    Sends messages and OpenAI tool definitions to the Chat Completions API\n",
        "    and returns the response.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of message dictionaries in the OpenAI format.\n",
        "        openai_tools: A list of tool definitions in the OpenAI format.\n",
        "        model: The name of the OpenAI model to use (default: gpt-4o-mini).\n",
        "        temperature: The sampling temperature (default: 0.7).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the response from the OpenAI API.\n",
        "        Returns an empty dictionary and prints an error message if the API call fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 2. Inside the function, use the openai.ChatCompletion.create method\n",
        "        # Using the newer openai library syntax (v1.0+)\n",
        "        # The older openai.ChatCompletion.create is deprecated in favor of client.chat.completions.create\n",
        "        # Let's use the newer client syntax\n",
        "        client = openai.OpenAI() # Assumes OPENAI_API_KEY is set as an environment variable\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=openai_tools,\n",
        "            tool_choice=\"auto\",  # auto lets the model decide whether to call a tool or respond\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        # The response object structure is slightly different in the new client\n",
        "        # Returning the full response object for now\n",
        "        return response\n",
        "\n",
        "    # 3. Include error handling (e.g., a try...except block) for the API call.\n",
        "    except openai.APIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the OpenAI API call: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Example usage (assuming 'openai_tools' is defined from the previous step)\n",
        "# and 'messages' is a list of dictionaries like [{\"role\": \"user\", \"content\": \"Your question here\"}]\n",
        "# Example messages setup:\n",
        "# messages_history = [{\"role\": \"user\", \"content\": \"What does Articles 9 of the OECD Model Tax Convention state?\"}]\n",
        "# if 'openai_tools' in globals() and openai_tools:\n",
        "#     print(\"Attempting to call chat_with_tools...\")\n",
        "#     api_response = chat_with_tools(messages_history, openai_tools)\n",
        "#     print(\"\\nAPI Response:\")\n",
        "#     print(api_response)\n",
        "# else:\n",
        "#     print(\"openai_tools not defined. Skipping chat_with_tools example call.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78669f73"
      },
      "source": [
        "## Implement tool calling handling\n",
        "\n",
        "### Subtask:\n",
        "Within the chat loop, check the model's response for `tool_calls`. If present, execute the corresponding tool (query the appropriate LlamaIndex engine) and send the tool output back to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a179c11"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the main chat loop that iteratively calls the chat_with_tools function, checks for tool_calls in the response, executes the tools using the corresponding LlamaIndex query engines, appends the tool outputs to the messages history, and continues the conversation until the model provides a final answer without tool calls. This combines steps 1-10 of the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be04c08e"
      },
      "source": [
        "import json # Import json if not already imported in this block\n",
        "import time # Import time for potential delays\n",
        "import openai # Import openai\n",
        "\n",
        "# Assume necessary variables and functions are defined from previous successful cells:\n",
        "# OECD_index, form990_guidelines_index (if used), tools (LlamaIndex QueryEngineTool instances),\n",
        "# openai_tools (OpenAI tool definitions), chat_with_tools, cosine_similarity_reward (if needed elsewhere)\n",
        "# Settings (LlamaIndex) are assumed to be configured.\n",
        "\n",
        "# Ensure query engines corresponding to the tools are accessible\n",
        "# Assuming they are defined from prior steps, e.g.:\n",
        "# OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "# form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Create a dictionary mapping tool names to query engines for easy lookup\n",
        "# Ensure OECD_engine and form990_guidelines_engine are defined and loaded from index\n",
        "query_engine_map = {}\n",
        "if 'OECD_index' in globals() and OECD_index is not None:\n",
        "    try:\n",
        "        query_engine_map[\"OECD_QueryEngineTool_2022\"] = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "        print(\"Created OECD query engine.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating OECD query engine: {e}\")\n",
        "else:\n",
        "    print(\"OECD_index not found or loaded. OECD tool execution will fail.\")\n",
        "\n",
        "# Assuming form990_guidelines_index is also loaded and available\n",
        "if 'form990_guidelines_index' in globals() and form990_guidelines_index is not None:\n",
        "     try:\n",
        "          query_engine_map[\"form990_2022\"] = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "          print(\"Created Form990 query engine.\")\n",
        "     except Exception as e:\n",
        "          print(f\"Error creating Form990 query engine: {e}\")\n",
        "else:\n",
        "     print(\"form990_guidelines_index not found or loaded. Form990 tool execution will fail.\")\n",
        "\n",
        "\n",
        "# 1. Initialize the conversation with a system message and the user's first question\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are an assistant that provides answers to questions on OECD and Form990 using the available tools. Answer as accurately as possible based on the tool outputs. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\"}]\n",
        "user_question = \"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\"\n",
        "# user_question = \"What does Articles 9 and 25 of the OECD Model Tax Convention state?\" # Example question for OECD tool\n",
        "messages.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "# Ensure openai_tools is defined from the previous step (conversion of LlamaIndex tools)\n",
        "if 'openai_tools' not in globals() or not openai_tools:\n",
        "    print(\"OpenAI tool definitions ('openai_tools') not found or are empty. Cannot proceed with tool-using chat loop.\")\n",
        "    # Define dummy tools to prevent crash if previous cell failed, but tool calls won't work\n",
        "    openai_tools = [{\"type\": \"function\", \"function\": {\"name\": \"dummy_tool\", \"description\": \"A dummy tool.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"input\": {\"type\": \"string\"}}, \"required\": [\"input\"]}}}]\n",
        "\n",
        "\n",
        "# --- Main Chat Loop ---\n",
        "print(f\"Starting chat loop for question: {user_question}\")\n",
        "\n",
        "# Keep track of the number of turns to prevent infinite loops\n",
        "max_turns = 10\n",
        "turn_count = 0\n",
        "final_response_content = None # Variable to store the final answer from the model\n",
        "\n",
        "while turn_count < max_turns:\n",
        "    turn_count += 1\n",
        "    print(f\"\\n--- Turn {turn_count} ---\")\n",
        "\n",
        "    # 2. Send messages to the model using chat_with_tools\n",
        "    # Ensure chat_with_tools function is defined from a previous cell\n",
        "    api_response = chat_with_tools(messages, openai_tools)\n",
        "\n",
        "    # Check if the API call was successful and has choices\n",
        "    if not api_response or not hasattr(api_response, 'choices') or not api_response.choices:\n",
        "        print(\"API call failed or returned no choices.\")\n",
        "        break # Exit loop if API call fails\n",
        "\n",
        "    # Extract the message from the response\n",
        "    response_message = api_response.choices[0].message\n",
        "    print(f\"Model response received (Role: {response_message.role})\")\n",
        "\n",
        "    # 3. Check if the response contains tool_calls\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        print(\"Model requested tool calls.\")\n",
        "        # Append the model's message (requesting tools) to the messages history\n",
        "        messages.append(response_message)\n",
        "\n",
        "        # 4. Iterate through each tool call\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_args_str = tool_call.function.arguments\n",
        "            tool_call_id = tool_call.id # Get the tool call ID\n",
        "\n",
        "            print(f\"  Tool call requested: {function_name} with args: {function_args_str}\")\n",
        "\n",
        "            # 5. Parse the arguments string\n",
        "            try:\n",
        "                function_args = json.loads(function_args_str)\n",
        "                tool_input = function_args.get(\"input\") # Extract the input argument\n",
        "                if tool_input is None:\n",
        "                     print(f\"Warning: 'input' argument not found in tool call args for {function_name}.\")\n",
        "                     tool_input = \"\" # Use empty string if input is missing\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding tool call arguments JSON for {function_name}: {function_args_str}\")\n",
        "                tool_input = \"\" # Use empty string or handle as error\n",
        "\n",
        "            # 6. Identify and execute the corresponding LlamaIndex query engine\n",
        "            query_engine = query_engine_map.get(function_name)\n",
        "            tool_output = \"\" # Initialize tool output\n",
        "\n",
        "            if query_engine:\n",
        "                print(f\"  Executing tool: {function_name} with input: '{tool_input}'\")\n",
        "                try:\n",
        "                    # Execute the query using the LlamaIndex engine\n",
        "                    llama_response = query_engine.query(tool_input)\n",
        "                    tool_output = str(llama_response) # Convert response to string\n",
        "                    print(f\"  Tool execution successful. Output snippet: '{tool_output[:100]}...'\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error executing LlamaIndex tool '{function_name}': {e}\")\n",
        "                    tool_output = f\"Error executing tool: {e}\" # Provide error message as tool output\n",
        "            else:\n",
        "                print(f\"  Error: No LlamaIndex query engine found for tool name '{function_name}'.\")\n",
        "                tool_output = f\"Error: Tool '{function_name}' not supported or found.\"\n",
        "\n",
        "            # 7. Format the output from the LlamaIndex query engine into the required format for the API\n",
        "            tool_message = {\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_call_id\": tool_call_id, # Link the tool output to the specific tool call\n",
        "                \"content\": tool_output,\n",
        "            }\n",
        "\n",
        "            # 8. Append this tool message to the messages history\n",
        "            messages.append(tool_message)\n",
        "            print(\"  Tool output appended to messages history.\")\n",
        "\n",
        "        # 9. Send the updated messages history back to the chat_with_tools function\n",
        "        # The loop continues, and the next iteration will send the updated 'messages'\n",
        "        # The model will then process the tool outputs and generate a response.\n",
        "\n",
        "    else:\n",
        "        # 10. If the model responds without tool_calls, it's the final answer\n",
        "        print(\"Model responded without tool calls. This is the final answer.\")\n",
        "        final_response_content = response_message.content\n",
        "        messages.append(response_message) # Append the final response to history\n",
        "        break # Exit the loop\n",
        "\n",
        "    # Add a small delay to avoid hitting rate limits too quickly during development\n",
        "    time.sleep(1)\n",
        "\n",
        "# After the loop, print the final answer if available\n",
        "if final_response_content:\n",
        "    print(\"\\n--- Final Answer ---\")\n",
        "    print(final_response_content)\n",
        "elif turn_count >= max_turns:\n",
        "    print(\"\\n--- Chat loop ended due to reaching max turns ---\")\n",
        "    # Optionally print the last message from the model\n",
        "    if messages:\n",
        "        print(\"Last message from model:\")\n",
        "        print(messages[-1])\n",
        "\n",
        "\n",
        "print(\"\\nChat loop finished.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68afd752"
      },
      "source": [
        "## Handle final response\n",
        "\n",
        "### Subtask:\n",
        "Process the model's final response after tool execution to extract the answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c7e5b3"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the final answer string from the `content` attribute of the `response_message` object after the loop concludes, and store it in `final_answer_content`. Print this variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdccbc1"
      },
      "source": [
        "# Assume the chat loop from the previous step has just completed.\n",
        "# The `response_message` variable holds the last message received from the model.\n",
        "# The `final_response_content` variable was intended to store the final answer.\n",
        "# The `messages` list holds the entire conversation history.\n",
        "\n",
        "# Check if the loop terminated because a final answer was received (i.e., no tool calls in the last message)\n",
        "# and if the last message is not empty.\n",
        "if response_message and not response_message.tool_calls and response_message.content:\n",
        "    final_answer_content = response_message.content\n",
        "    print(\"\\n--- Final Answer (Extracted after loop) ---\")\n",
        "    print(final_answer_content)\n",
        "elif turn_count >= max_turns:\n",
        "    print(\"\\n--- Chat loop ended due to reaching max turns ---\")\n",
        "    # If max turns reached, the last message might contain a partial answer or just model thoughts.\n",
        "    # We can still try to extract content if available, but it might not be a complete final answer.\n",
        "    if response_message and response_message.content:\n",
        "         final_answer_content = response_message.content\n",
        "         print(\"Last message content:\")\n",
        "         print(final_answer_content)\n",
        "    else:\n",
        "         final_answer_content = \"Chat loop ended without a final answer.\"\n",
        "         print(final_answer_content)\n",
        "else:\n",
        "    # Handle other potential loop exit conditions or errors\n",
        "    final_answer_content = \"Chat loop terminated unexpectedly without a final answer.\"\n",
        "    print(final_answer_content)\n",
        "\n",
        "# The final_answer_content variable now holds the extracted final answer or an informative message.\n",
        "# It can be used for further processing or evaluation.\n",
        "\n",
        "# Note: The previous cell's code already included printing logic after the loop.\n",
        "# This cell explicitly focuses on ensuring the extraction and storage in `final_answer_content`.\n",
        "# The printing is included here to demonstrate the result of the extraction."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
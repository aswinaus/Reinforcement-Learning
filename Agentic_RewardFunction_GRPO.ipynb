{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Reinforcement-Learning/blob/main/Agentic_RewardFunction_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpEGNalHDvqV"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ul8zj9UhETD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "rRxaZjaZEXWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "D38Nn8M1Eey2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "ZiGL3yLfF-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ],
      "metadata": {
        "id": "f-ZbbEzxGSzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afbd2a"
      },
      "source": [
        "# Task\n",
        "Calculate rewards based on cosine similarity, explain how to update the policy directly in GRPO for the current situation, and define the policy network for this example, specifically referring to Group Relative Policy Optimization (GRPO) in the context of a RAG system where the reward is based on cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5a3b13"
      },
      "source": [
        "## Understand the role of the policy network\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of the RAG system the policy network will control (e.g., retrieval parameters, generation parameters, or both).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0389c9"
      },
      "source": [
        "## Choose a suitable neural network architecture\n",
        "\n",
        "### Subtask:\n",
        "Select a type of neural network (e.g., a feedforward network, recurrent network, or transformer-based model) that can process the input question and output the control signals for the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c336904d"
      },
      "source": [
        "**Reasoning**:\n",
        "To select a neural network architecture suitable for processing text input (the question) and outputting control signals for the RAG system, considering the sequential nature of text and the need to capture semantic meaning, a transformer-based model is a good choice due to its effectiveness in handling sequential data and complex relationships within text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b210d0"
      },
      "source": [
        "# Explanation of the chosen neural network architecture\n",
        "\n",
        "# For this task, where the input is a text question and the output needs to be control signals\n",
        "# for a RAG system (e.g., parameters for retrieval or generation), a transformer-based model\n",
        "# is a suitable choice.\n",
        "\n",
        "# Justification:\n",
        "# 1. Text Input: Transformers are specifically designed to process sequential data like text.\n",
        "# 2. Semantic Understanding: They excel at capturing long-range dependencies and semantic relationships\n",
        "#    within the input question, which is crucial for determining appropriate RAG control signals.\n",
        "# 3. Contextual Embeddings: Transformer models generate rich contextual embeddings of the input text,\n",
        "#    allowing the policy network to make informed decisions based on the nuances of the question.\n",
        "# 4. Adaptability: Transformer architectures can be adapted with output layers to produce numerical\n",
        "#    or categorical control signals as needed by the RAG system.\n",
        "\n",
        "# Examples of transformer-based models that could be adapted for this purpose include\n",
        "# models based on BERT, RoBERTa, or smaller, task-specific transformer variants.\n",
        "# The specific output layer would depend on the exact control signals being predicted (e.g.,\n",
        "# a dense layer for numerical parameters or a softmax layer for categorical choices).\n",
        "\n",
        "# This choice aligns with the task of learning a policy based on textual input to influence\n",
        "# the RAG system's behavior for better performance according to the defined reward."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2e884"
      },
      "source": [
        "## Define the input and output layers\n",
        "\n",
        "### Subtask:\n",
        "Define the format of the input (the question) and the output (the parameters or actions that influence the RAG system) for the chosen transformer-based policy network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c5cf8a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the input and output format of the chosen transformer-based policy network based on the previous subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a702d66a"
      },
      "source": [
        "# 1. Input Layer Format:\n",
        "# The input to the policy network will be the user's question, which is a string of text.\n",
        "# Before being fed into the transformer layers, this text will undergo standard NLP preprocessing steps:\n",
        "# - Tokenization: The text will be broken down into a sequence of tokens (words or sub-word units) using a tokenizer appropriate for the chosen transformer model (e.g., WordPiece for BERT, BPE for RoBERTa).\n",
        "# - Embedding: The sequence of tokens will be converted into a sequence of numerical embeddings. Transformer models typically use learned token embeddings, positional embeddings (to capture token order), and potentially segment embeddings. The input to the transformer layers will be a tensor of shape (batch_size, sequence_length, embedding_dim), where:\n",
        "#   - batch_size: The number of questions processed in parallel.\n",
        "#   - sequence_length: The maximum number of tokens in a question (padded or truncated).\n",
        "#   - embedding_dim: The dimensionality of the token embeddings.\n",
        "\n",
        "# 2. Output Layer(s) Format:\n",
        "# The output layer(s) of the policy network will produce control signals for the RAG system. Based on the understanding that the policy network controls retrieval parameters (like similarity_top_k) and potentially influences generation, the output could be structured as follows:\n",
        "# - For a numerical parameter like `similarity_top_k`: A single dense layer with one output neuron, potentially followed by an activation function (e.g., ReLU to ensure non-negativity) and possibly scaled to a reasonable range. The output would be a tensor of shape (batch_size, 1).\n",
        "# - For influencing generation (less direct control in this setup, but conceptually): This could be represented as a vector influencing attention mechanisms or providing context to the generation model. However, focusing on retrieval parameters as the primary policy output in this GRPO context is more straightforward.\n",
        "# - For simplicity and direct control over a key retrieval parameter, let's define the output as a single numerical value representing `similarity_top_k`. The output layer will be a dense layer with 1 output neuron.\n",
        "\n",
        "# Therefore, the output of the policy network will be a tensor of shape (batch_size, 1), representing the predicted value for `similarity_top_k` for each question in the batch.\n",
        "\n",
        "# 3. Interpretation and Usage of the Policy Network's Output:\n",
        "# The output of the policy network (the predicted `similarity_top_k` value) will be used to configure the retrieval step of the RAG system for the given question.\n",
        "# - During training: The predicted `similarity_top_k` will be used to perform retrieval. The retrieved context, along with the question, will then be passed to the generation model to produce an answer. This answer will be compared to the ground truth to calculate the cosine similarity reward. This reward will be used by the GRPO algorithm to update the policy network's weights, encouraging it to predict `similarity_top_k` values that lead to higher rewards.\n",
        "# - During inference: The policy network will predict `similarity_top_k` for a new question, and this value will be used directly in the retrieval process to gather context for generating the final answer.\n",
        "# The predicted numerical output for `similarity_top_k` might need to be post-processed (e.g., rounded to an integer, clipped to a valid range) before being used by the RAG system's retriever."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ece352"
      },
      "source": [
        "## Implement the policy network\n",
        "\n",
        "### Subtask:\n",
        "Implement the policy network using a deep learning framework. This involves defining the transformer layers and the output layer(s) based on the input and output formats defined in the previous steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20adbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy network using PyTorch, defining the transformer layers and the output layer as specified in previous steps.\n",
        "\n",
        "In essence, this network takes a question, processes it through a pre-trained transformer to understand its context, and then uses a simple linear layer to predict a non-negative numerical value intended to represent the optimal similarity_top_k for retrieving documents for that question.\n",
        "\n",
        "In the context of Reinforcement Learning (RL), a policy network is a neural network that learns to map states (in our case, the user's question) to actions (the parameters or decisions that control the RAG system).\n",
        "\n",
        "**State:** The input is the user's question. The policy network processes this question to understand its meaning and context.\n",
        "\n",
        "**Action:** The output of the policy network is a value (or values) that influences how the RAG system operates. In the code we just discussed, the policy network's action space was initially simplified to predicting a single value: similarity_top_k, which determines how many relevant documents are retrieved. In the modified code, it predicts parameters for a distribution from which similarity_top_k is sampled.\n",
        "\n",
        "The goal of training the policy network using an algorithm like GRPO is to adjust its internal parameters (the weights and biases of the neural network) so that, when presented with a question, it predicts/samples actions (like a specific similarity_top_k) that lead to higher rewards (in our case, higher cosine similarity between the generated answer and the ground truth).\n",
        "\n",
        "So, the policy network's role is to learn the optimal strategy for configuring the RAG system based on the input question to maximize the desired outcome (answer quality, measured by the reward).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ff9198"
      },
      "source": [
        "# Task\n",
        "**Implement the policy update rule for the GRPO algorithm using cosine similarity as the reward signal to adjust the policy network's parameters in the provided notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a4c096"
      },
      "source": [
        "## Modify policy network output\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `RAGPolicyNetwork` to output parameters for a distribution over `similarity_top_k`, such as the mean and log-variance of a Gaussian distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fb1c21"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the RAGPolicyNetwork class to output parameters for a Gaussian distribution (mean and log-variance) over `similarity_top_k`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515fe81f"
      },
      "source": [
        "## Implement action sampling and log probability calculation\n",
        "\n",
        "### Subtask:\n",
        "Implement functions to sample `similarity_top_k` from the Gaussian distribution predicted by the policy network and calculate the log probability of the sampled action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71623969"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement functions to sample the action (similarity_top_k) from the predicted Gaussian distribution and calculate the log probability of the sampled action, using the predicted mean and log-variance from the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60f1e206"
      },
      "source": [
        "## Implement baseline calculation\n",
        "\n",
        "### Subtask:\n",
        "Create a function to calculate a baseline for the rewards, such as the average reward in a batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160b14ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to calculate the mean of a list or tensor of rewards to be used as a baseline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422375d8"
      },
      "source": [
        "## Set up training loop\n",
        "\n",
        "### Subtask:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8558acf5"
      },
      "source": [
        "**Reasoning**:\n",
        "Structure a training loop that iterates through the dataset, performs forward passes with the policy network, executes the RAG system with sampled actions, calculates rewards and advantages, and computes the policy gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8479c2f2"
      },
      "source": [
        "## Implement policy update\n",
        "\n",
        "### Subtask:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using an optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0307182"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the calculated policy gradient to update the parameters of the policy network using the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03bb1ac3"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "After training, evaluate the performance of the policy-controlled RAG system and refine the implementation or hyperparameters as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81edce18"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the policy-controlled RAG system after the training loop. This involves setting the policy network to evaluation mode, using a dataset (can be the same as training for demonstration), iterating through questions, predicting/sampling `similarity_top_k`, executing the RAG query, calculating the cosine similarity reward, and finally reporting the average reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287a48b1"
      },
      "source": [
        "# Task\n",
        "Implement observability metrics for the training process of a policy network using Weights and Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0da520a7"
      },
      "source": [
        "## Identify key metrics\n",
        "\n",
        "### Subtask:\n",
        "Determine which metrics are most important to track for monitoring the training process of the policy network (e.g., epoch number, batch number, average policy loss per batch, average reward per batch, average predicted similarity_top_k per batch, average advantage per batch).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e8b7ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify and list the key metrics for monitoring the training process of the policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459dfae1"
      },
      "source": [
        "## Integrate weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Install the `wandb` library and initialize a Weights & Biases run at the beginning of the training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb8ada8"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the wandb library and initialize a Weights & Biases run at the beginning of the training script as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ca260e"
      },
      "source": [
        "## Log hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Log the training hyperparameters (e.g., learning rate, batch size, number of epochs) to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22da477d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a dictionary containing the training hyperparameters and log it to the initialized Weights & Biases run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de2b5e9"
      },
      "source": [
        "## Integrate metric calculation into the training loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to calculate the chosen metrics for each batch and/or epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9543160b"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to calculate the selected metrics for each batch and epoch, including average policy loss, average reward, average predicted similarity_top_k, average advantage, average mean, and average log-variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b06792f"
      },
      "source": [
        "## Log metrics to weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop. This will typically involve using `wandb.log()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48c0ca5"
      },
      "source": [
        "**Reasoning**:\n",
        "Add code to log the calculated metrics to Weights & Biases within the training loop as requested by the subtask. This involves using `wandb.log()` for both batch and epoch metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbaad26"
      },
      "source": [
        "## Visualize and analyze metrics in weights & biases\n",
        "\n",
        "### Subtask:\n",
        "Use the Weights & Biases dashboard to visualize the logged metrics, monitor training progress, and identify potential issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b145d0fd"
      },
      "source": [
        "## Refine logging and metrics\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked or the logging frequency for better insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d72db7"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis in Weights & Biases, refine the set of metrics being tracked and their logging frequency in the training code. The current metrics are informative, but batch-level metrics can be noisy. Epoch-level metrics provide a smoother view of overall progress. We will keep logging both but ensure epoch metrics are clearly distinguished. We can also consider adding the standard deviation of `similarity_top_k` predictions at the epoch level to see how the policy's uncertainty evolves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ],
      "metadata": {
        "id": "vouyCtSmrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop includes the actual RAG execution using the OECD_index and the cosine_similarity_reward function to calculate the reward based on the generated answer and the ground truth for each question in the batch. This means the policy network is being trained using real rewards derived from the RAG system's performance."
      ],
      "metadata": {
        "id": "y0UM0l3tbSZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of training results logged in W&B**\n",
        "\n",
        "**Epoch 27/100, Avg Loss: 0.0183, Avg Reward: 0.3199, Avg Predicted Top K: 2.80, Predicted Top K Std: 0.40**\n",
        "\n",
        "**Epoch** 27/100: This indicates that the training is currently on the 27th iteration (epoch) out of a planned total of 100 epochs.\n",
        "\n",
        "**Avg Loss**: 0.0183: This is the average policy loss calculated across all the batches in Epoch 27. In reinforcement learning, the policy loss is a value that the optimization algorithm (like the one used in GRPO) tries to minimize. A **lower loss** generally suggests that the **policy** is being **updated** in a **direction** that is expected to **increase** **rewards**. The value 0.0183 is the magnitude of this average loss for this specific epoch.\n",
        "\n",
        "**Avg Reward**: 0.3199: This is the **average cosine similarity reward** obtained during Epoch 27. This reward is calculated by running the RAG system with the actions (predicted similarity_top_k values) sampled by the policy network for each question in the dataset and then comparing the generated answers to the ground truth. An **average** **reward** of 0.3199 means that, on **average** across all samples in this epoch, the **generated** **answers** had a **cosine** **similarity** of approximately 0.32 with their **respective** **ground** **truth** **answers**. **Higher** values indicate **better** **performance** in terms of **semantic** **similarity**.\n",
        "\n",
        "**Avg Predicted Top K**: 2.80: This is the average value of similarity_top_k predicted (or sampled and then processed into an integer) by the policy network across all questions in Epoch 27. This **metric** **tells** you what the **policy** **network** is generally **choosing** for the **number** of **documents** to **retrieve**. An **average** of 2.80 suggests the **policy** is typically **selecting** around 3 **documents**.\n",
        "\n",
        "**Predicted Top K Std**: 0.40: This is the standard deviation of the similarity_top_k values predicted (or sampled and processed) by the policy network across all questions in Epoch 27. The standard deviation measures the dispersion or spread of the predicted values. A standard deviation of 0.40 indicates that the predicted similarity_top_k values in this epoch were relatively close to the average (2.80), meaning the policy's predictions for similarity_top_k didn't vary widely within this epoch."
      ],
      "metadata": {
        "id": "JpnYg3dccV0v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de918f7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The training process successfully logged key metrics at both the batch and epoch levels to Weights & Biases, including policy loss, average reward, average predicted `similarity_top_k`, average advantage, average mean, and average log-variance of the predicted distribution.\n",
        "*   Hyperparameters such as learning rate (1e-4), batch size (8), and number of epochs (100) were successfully logged to the Weights & Biases config.\n",
        "*   The training loop executed for 100 epochs, with console output confirming the progress and epoch-average loss and reward.\n",
        "*   A new epoch-level metric, the standard deviation of the predicted `similarity_top_k`, was successfully added and logged to provide insight into the variability of the policy's actions.\n",
        "\n",
        "### Completed Steps\n",
        "\n",
        "*   Visualized the logged metrics in the Weights & Biases dashboard to analyze trends, identify correlations between metrics (e.g., reward and predicted top\\_k), and diagnose potential training issues such as instability or convergence problems.\n",
        "*   Implement the actual RAG system reward calculation to replace the dummy reward function, allowing the policy to learn based on real retrieval performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9915760"
      },
      "source": [
        "# Task\n",
        "Extend the provided Python code for training a RAG policy to maintain and train a group of policies using a modified training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74c9789d"
      },
      "source": [
        "## Modify policy network management - GRPO implementation\n",
        "\n",
        "### Subtask:\n",
        "Change the code to create and manage a list or collection of `RAGPolicyNetwork` instances instead of just one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a5ae7d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to replace the single policy network instance with a list of multiple policy network instances. This involves defining the number of policies, creating a container for them (a list or ModuleList), and instantiating the specified number of policies within a loop. This needs to be done in a code block that replaces the current instantiation of the single policy network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624abfb5"
      },
      "source": [
        "## Adapt data collection - GRPO Comparing the performance of policies within the group.\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to collect data (questions, actions, rewards, log probabilities) for each policy in the group over an iteration. This might involve running each policy on the same batch or different batches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89200182"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the training loop structure to iterate through the `policy_group`, process a batch of data for each policy, perform a forward pass, sample actions and calculate log probabilities, execute the RAG system, calculate rewards, and store the results for each policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "425d12f2"
      },
      "source": [
        "## Implement group performance evaluation\n",
        "\n",
        "### Subtask:\n",
        "Add logic to evaluate the performance of each policy in the group based on the collected rewards. This could be the average reward over the data collected for that policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9da2dd81"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to evaluate the performance of each policy in the group based on the collected rewards within the training loop. This involves calculating the average reward for each policy after collecting data across the dataset for the epoch. Store these average rewards and identify the best-performing policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c874787"
      },
      "source": [
        "## Adapt policy update rule\n",
        "\n",
        "### Subtask:\n",
        "Modify the policy update step to incorporate the group information, potentially using the average reward or relative performance within the group to adjust gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cad60247"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy update step for each policy in the group after data collection and evaluation, calculating the individual policy loss, zeroing gradients, performing the backward pass, and updating the optimizer, and logging the policy loss to Weights & Biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c6cd75"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the Weights & Biases logging for policy-specific and group-level metrics as requested by the subtask. This involves modifying the training loop to collect and log these metrics at the end of each epoch. I will ensure all necessary variables and functions from previous cells are included in this code block to make it runnable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09e3456e"
      },
      "source": [
        "# Task\n",
        "Extend the provided Python code to implement a basic version of Grouped Relative Policy Optimization (GRPO). This involves maintaining a group of policies, comparing their performance, and using a trust region-like approach for policy updates within the group. Ensure unnecessary code is removed before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14010c7c"
      },
      "source": [
        "## Modify policy network management\n",
        "\n",
        "### Subtask:\n",
        "Change the code to create and manage a list or collection of `RAGPolicyNetwork` instances instead of just one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eaebdb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create and manage a group of policy networks and their optimizers. This involves defining the number of policies, instantiating the `RAGPolicyNetwork` class multiple times, storing them in a list, and creating a corresponding list of optimizers. I will include all necessary imports and definitions from previous successful steps to make this code block runnable and self-contained as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448c6b72"
      },
      "source": [
        "# Task\n",
        "Extend the provided Python code to implement a training loop that maintains a group of policies, compares their performance, and uses a trust region method for policy updates, reflecting the principles of Group Relative Policy Optimization (GRPO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a12323"
      },
      "source": [
        "## Modify policy network management\n",
        "\n",
        "### Subtask:\n",
        "Change the code to create and manage a list or collection of `RAGPolicyNetwork` instances instead of just one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f2d6f3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create and manage a group of policy networks and their optimizers. This involves defining the number of policies, instantiating the RAGPolicyNetwork class multiple times, storing them in a list, and creating a corresponding list of optimizers. I will include all necessary imports and definitions from previous successful steps to make this code block runnable and self-contained as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3818f2e7"
      },
      "source": [
        "## Adapt policy update rule\n",
        "\n",
        "### Subtask:\n",
        "Modify the policy update step to incorporate the group information. This is where the \"Relative\" part of GRPO comes in. The update for each policy might depend on its performance relative to others in the group, or updates might be averaged across the group. (Note: A full GRPO implementation would involve more sophisticated trust region methods based on the group, but for this plan, we'll focus on managing the group and adapting the basic update).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9739246f"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy update step for each policy in the group after data collection and evaluation, calculating the individual policy loss, zeroing gradients, performing the backward pass, and updating the optimizer, and logging the policy loss to Weights & Biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0624b574"
      },
      "source": [
        "## Update logging\n",
        "\n",
        "### Subtask:\n",
        "Modify Weights & Biases logging to track metrics for each policy in the group, or group-level metrics (e.g., average reward of the best policy, average reward across the group).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004c1a52"
      },
      "source": [
        "# Task\n",
        "Extend the provided Python code to maintain a group of policies, adapt data collection for each policy in the group, implement group performance evaluation, and adapt the policy update rule to incorporate group information. Ensure Weights & Biases logging is updated to track metrics for each policy or group-level metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf7563ce"
      },
      "source": [
        "## Modify policy network management\n",
        "\n",
        "### Subtask:\n",
        "Change the code to create and manage a list or collection of `RAGPolicyNetwork` instances instead of just one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b605a43"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create and manage a group of policy networks and their optimizers. This involves defining the number of policies, instantiating the RAGPolicyNetwork class multiple times, storing them in a list, and creating a corresponding list of optimizers. I will include all necessary imports and definitions from previous successful steps to make this code block runnable and self-contained as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04538c40"
      },
      "source": [
        "**Reasoning**:\n",
        "Remove the deprecated OpenAIAssistantAgent import and instantiation from cell `hlIsonI9Hlx6` as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4b1a90"
      },
      "source": [
        "**Reasoning**:\n",
        "Remove the deprecated OpenAIAssistantAgent import and instantiation from cell `COLLlq4wJZZP` as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8910f4ff"
      },
      "source": [
        "## Define tools for chat completions api\n",
        "\n",
        "### Subtask:\n",
        "Adapt the existing `QueryEngineTool` instances to a format compatible with the Chat Completions API's `tools` parameter (list of function definitions).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af444278"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function `openai_tool_definition` to convert a QueryEngineTool to the OpenAI Chat Completions API tool format and then apply it to the existing tools.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c6adb5a"
      },
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "import json # Import the json module for schema definition\n",
        "\n",
        "# Redefine or ensure tools are available from previous successful cells\n",
        "# Assuming OECD_query_tool and Form990_query_tool are defined here or in a prior cell\n",
        "# Example definition if not already available:\n",
        "# OECD_engine = OECD_index.as_query_engine(similarity_top_k=3) # Assuming OECD_index is loaded\n",
        "# form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3) # Assuming form990_guidelines_index is loaded\n",
        "# OECD_query_tool = QueryEngineTool(\n",
        "#                       query_engine=OECD_engine,\n",
        "#                       metadata=ToolMetadata(\n",
        "#                           name=\"OECD_QueryEngineTool_2022\",\n",
        "#                           description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "#                       )\n",
        "#                     )\n",
        "# Form990_query_tool = QueryEngineTool(\n",
        "#                       query_engine=form990_guidelines_engine,\n",
        "#                       metadata=ToolMetadata(\n",
        "#                           name=\"form990_2022\",\n",
        "#                           description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "#                       )\n",
        "#                     )\n",
        "# tools = [OECD_query_tool, Form990_query_tool] # Ensure 'tools' list is defined\n",
        "\n",
        "# 1. Define a function openai_tool_definition\n",
        "def openai_tool_definition(query_engine_tool: QueryEngineTool) -> dict:\n",
        "    \"\"\"\n",
        "    Converts a LlamaIndex QueryEngineTool to the OpenAI Chat Completions API\n",
        "    tool format.\n",
        "\n",
        "    Args:\n",
        "        query_engine_tool: The QueryEngineTool instance.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the tool definition in OpenAI's format.\n",
        "    \"\"\"\n",
        "    # 4. Define the parameters schema\n",
        "    parameters_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"input\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The query string to pass to the tool.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"input\"],\n",
        "    }\n",
        "\n",
        "    # 2. Construct the dictionary for the tool definition\n",
        "    tool_definition = {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            # 3. Extract name and description from metadata\n",
        "            \"name\": query_engine_tool.metadata.name,\n",
        "            \"description\": query_engine_tool.metadata.description,\n",
        "            \"parameters\": parameters_schema,\n",
        "        },\n",
        "    }\n",
        "    return tool_definition\n",
        "\n",
        "# 5. Apply this function to existing tools\n",
        "# Ensure 'tools' list is available from previous cells\n",
        "# If not, you'd need to define OECD_query_tool and Form990_query_tool here\n",
        "# Assuming 'tools' is defined and contains OECD_query_tool and Form990_query_tool\n",
        "if 'tools' in globals() and isinstance(tools, list) and len(tools) > 0:\n",
        "    openai_tools = [openai_tool_definition(tool) for tool in tools]\n",
        "    print(\"OpenAI tool definitions created successfully:\")\n",
        "    # Print the created tool definitions for verification\n",
        "    print(json.dumps(openai_tools, indent=2))\n",
        "else:\n",
        "    print(\"Error: 'tools' list not found or is empty. Cannot create OpenAI tool definitions.\")\n",
        "    openai_tools = [] # Initialize as empty if tools are not available\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bacfe15"
      },
      "source": [
        "## Implement chat completions logic\n",
        "\n",
        "### Subtask:\n",
        "Create a function or class that uses `openai.ChatCompletion.create` (or the equivalent using the `openai` library) to send messages to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84c459f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `chat_with_tools` that uses `openai.ChatCompletion.create` (or the equivalent) to send messages and tools to the model and handle the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4893a3f9"
      },
      "source": [
        "import openai\n",
        "import os # Import os to access environment variables\n",
        "\n",
        "# Ensure the OpenAI API key is set\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
        "    # In a real application, you would handle this more robustly\n",
        "    # For this example, we'll proceed but the API call will fail.\n",
        "\n",
        "\n",
        "# 1. Define a Python function, for example chat_with_tools\n",
        "def chat_with_tools(messages: list[dict], openai_tools: list[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.7) -> dict:\n",
        "    \"\"\"\n",
        "    Sends messages and OpenAI tool definitions to the Chat Completions API\n",
        "    and returns the response.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of message dictionaries in the OpenAI format.\n",
        "        openai_tools: A list of tool definitions in the OpenAI format.\n",
        "        model: The name of the OpenAI model to use (default: gpt-4o-mini).\n",
        "        temperature: The sampling temperature (default: 0.7).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the response from the OpenAI API.\n",
        "        Returns an empty dictionary and prints an error message if the API call fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 2. Inside the function, use the openai.ChatCompletion.create method\n",
        "        # Using the newer openai library syntax (v1.0+)\n",
        "        # The older openai.ChatCompletion.create is deprecated in favor of client.chat.completions.create\n",
        "        # Let's use the newer client syntax\n",
        "        client = openai.OpenAI() # Assumes OPENAI_API_KEY is set as an environment variable\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=openai_tools,\n",
        "            tool_choice=\"auto\",  # auto lets the model decide whether to call a tool or respond\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        # The response object structure is slightly different in the new client\n",
        "        # Returning the full response object for now\n",
        "        return response\n",
        "\n",
        "    # 3. Include error handling (e.g., a try...except block) for the API call.\n",
        "    except openai.APIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the OpenAI API call: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Example usage (assuming 'openai_tools' is defined from the previous step)\n",
        "# and 'messages' is a list of dictionaries like [{\"role\": \"user\", \"content\": \"Your question here\"}]\n",
        "# Example messages setup:\n",
        "# messages_history = [{\"role\": \"user\", \"content\": \"What does Articles 9 of the OECD Model Tax Convention state?\"}]\n",
        "# if 'openai_tools' in globals() and openai_tools:\n",
        "#     print(\"Attempting to call chat_with_tools...\")\n",
        "#     api_response = chat_with_tools(messages_history, openai_tools)\n",
        "#     print(\"\\nAPI Response:\")\n",
        "#     print(api_response)\n",
        "# else:\n",
        "#     print(\"openai_tools not defined. Skipping chat_with_tools example call.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78669f73"
      },
      "source": [
        "## Implement tool calling handling\n",
        "\n",
        "### Subtask:\n",
        "Within the chat loop, check the model's response for `tool_calls`. If present, execute the corresponding tool (query the appropriate LlamaIndex engine) and send the tool output back to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a179c11"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the main chat loop that iteratively calls the chat_with_tools function, checks for tool_calls in the response, executes the tools using the corresponding LlamaIndex query engines, appends the tool outputs to the messages history, and continues the conversation until the model provides a final answer without tool calls. This combines steps 1-10 of the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be04c08e"
      },
      "source": [
        "import json # Import json if not already imported in this block\n",
        "import time # Import time for potential delays\n",
        "import openai # Import openai\n",
        "\n",
        "# Assume necessary variables and functions are defined from previous successful cells:\n",
        "# OECD_index, form990_guidelines_index (if used), tools (LlamaIndex QueryEngineTool instances),\n",
        "# openai_tools (OpenAI tool definitions), chat_with_tools, cosine_similarity_reward (if needed elsewhere)\n",
        "# Settings (LlamaIndex) are assumed to be configured.\n",
        "\n",
        "# Ensure query engines corresponding to the tools are accessible\n",
        "# Assuming they are defined from prior steps, e.g.:\n",
        "# OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "# form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Create a dictionary mapping tool names to query engines for easy lookup\n",
        "# Ensure OECD_engine and form990_guidelines_engine are defined and loaded from index\n",
        "query_engine_map = {}\n",
        "if 'OECD_index' in globals() and OECD_index is not None:\n",
        "    try:\n",
        "        query_engine_map[\"OECD_QueryEngineTool_2022\"] = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "        print(\"Created OECD query engine.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating OECD query engine: {e}\")\n",
        "else:\n",
        "    print(\"OECD_index not found or loaded. OECD tool execution will fail.\")\n",
        "\n",
        "# Assuming form990_guidelines_index is also loaded and available\n",
        "if 'form990_guidelines_index' in globals() and form990_guidelines_index is not None:\n",
        "     try:\n",
        "          query_engine_map[\"form990_2022\"] = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "          print(\"Created Form990 query engine.\")\n",
        "     except Exception as e:\n",
        "          print(f\"Error creating Form990 query engine: {e}\")\n",
        "else:\n",
        "     print(\"form990_guidelines_index not found or loaded. Form990 tool execution will fail.\")\n",
        "\n",
        "\n",
        "# 1. Initialize the conversation with a system message and the user's first question\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are an assistant that provides answers to questions on OECD and Form990 using the available tools. Answer as accurately as possible based on the tool outputs. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\"}]\n",
        "user_question = \"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\"\n",
        "# user_question = \"What does Articles 9 and 25 of the OECD Model Tax Convention state?\" # Example question for OECD tool\n",
        "messages.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "# Ensure openai_tools is defined from the previous step (conversion of LlamaIndex tools)\n",
        "if 'openai_tools' not in globals() or not openai_tools:\n",
        "    print(\"OpenAI tool definitions ('openai_tools') not found or are empty. Cannot proceed with tool-using chat loop.\")\n",
        "    # Define dummy tools to prevent crash if previous cell failed, but tool calls won't work\n",
        "    openai_tools = [{\"type\": \"function\", \"function\": {\"name\": \"dummy_tool\", \"description\": \"A dummy tool.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"input\": {\"type\": \"string\"}}, \"required\": [\"input\"]}}}]\n",
        "\n",
        "\n",
        "# --- Main Chat Loop ---\n",
        "print(f\"Starting chat loop for question: {user_question}\")\n",
        "\n",
        "# Keep track of the number of turns to prevent infinite loops\n",
        "max_turns = 10\n",
        "turn_count = 0\n",
        "final_response_content = None # Variable to store the final answer from the model\n",
        "\n",
        "while turn_count < max_turns:\n",
        "    turn_count += 1\n",
        "    print(f\"\\n--- Turn {turn_count} ---\")\n",
        "\n",
        "    # 2. Send messages to the model using chat_with_tools\n",
        "    # Ensure chat_with_tools function is defined from a previous cell\n",
        "    api_response = chat_with_tools(messages, openai_tools)\n",
        "\n",
        "    # Check if the API call was successful and has choices\n",
        "    if not api_response or not hasattr(api_response, 'choices') or not api_response.choices:\n",
        "        print(\"API call failed or returned no choices.\")\n",
        "        break # Exit loop if API call fails\n",
        "\n",
        "    # Extract the message from the response\n",
        "    response_message = api_response.choices[0].message\n",
        "    print(f\"Model response received (Role: {response_message.role})\")\n",
        "\n",
        "    # 3. Check if the response contains tool_calls\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        print(\"Model requested tool calls.\")\n",
        "        # Append the model's message (requesting tools) to the messages history\n",
        "        messages.append(response_message)\n",
        "\n",
        "        # 4. Iterate through each tool call\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_args_str = tool_call.function.arguments\n",
        "            tool_call_id = tool_call.id # Get the tool call ID\n",
        "\n",
        "            print(f\"  Tool call requested: {function_name} with args: {function_args_str}\")\n",
        "\n",
        "            # 5. Parse the arguments string\n",
        "            try:\n",
        "                function_args = json.loads(function_args_str)\n",
        "                tool_input = function_args.get(\"input\") # Extract the input argument\n",
        "                if tool_input is None:\n",
        "                     print(f\"Warning: 'input' argument not found in tool call args for {function_name}.\")\n",
        "                     tool_input = \"\" # Use empty string if input is missing\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding tool call arguments JSON for {function_name}: {function_args_str}\")\n",
        "                tool_input = \"\" # Use empty string or handle as error\n",
        "\n",
        "            # 6. Identify and execute the corresponding LlamaIndex query engine\n",
        "            query_engine = query_engine_map.get(function_name)\n",
        "            tool_output = \"\" # Initialize tool output\n",
        "\n",
        "            if query_engine:\n",
        "                print(f\"  Executing tool: {function_name} with input: '{tool_input}'\")\n",
        "                try:\n",
        "                    # Execute the query using the LlamaIndex engine\n",
        "                    llama_response = query_engine.query(tool_input)\n",
        "                    tool_output = str(llama_response) # Convert response to string\n",
        "                    print(f\"  Tool execution successful. Output snippet: '{tool_output[:100]}...'\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error executing LlamaIndex tool '{function_name}': {e}\")\n",
        "                    tool_output = f\"Error executing tool: {e}\" # Provide error message as tool output\n",
        "            else:\n",
        "                print(f\"  Error: No LlamaIndex query engine found for tool name '{function_name}'.\")\n",
        "                tool_output = f\"Error: Tool '{function_name}' not supported or found.\"\n",
        "\n",
        "            # 7. Format the output from the LlamaIndex query engine into the required format for the API\n",
        "            tool_message = {\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_call_id\": tool_call_id, # Link the tool output to the specific tool call\n",
        "                \"content\": tool_output,\n",
        "            }\n",
        "\n",
        "            # 8. Append this tool message to the messages history\n",
        "            messages.append(tool_message)\n",
        "            print(\"  Tool output appended to messages history.\")\n",
        "\n",
        "        # 9. Send the updated messages history back to the chat_with_tools function\n",
        "        # The loop continues, and the next iteration will send the updated 'messages'\n",
        "        # The model will then process the tool outputs and generate a response.\n",
        "\n",
        "    else:\n",
        "        # 10. If the model responds without tool_calls, it's the final answer\n",
        "        print(\"Model responded without tool calls. This is the final answer.\")\n",
        "        final_response_content = response_message.content\n",
        "        messages.append(response_message) # Append the final response to history\n",
        "        break # Exit the loop\n",
        "\n",
        "    # Add a small delay to avoid hitting rate limits too quickly during development\n",
        "    time.sleep(1)\n",
        "\n",
        "# After the loop, print the final answer if available\n",
        "if final_response_content:\n",
        "    print(\"\\n--- Final Answer ---\")\n",
        "    print(final_response_content)\n",
        "elif turn_count >= max_turns:\n",
        "    print(\"\\n--- Chat loop ended due to reaching max turns ---\")\n",
        "    # Optionally print the last message from the model\n",
        "    if messages:\n",
        "        print(\"Last message from model:\")\n",
        "        print(messages[-1])\n",
        "\n",
        "\n",
        "print(\"\\nChat loop finished.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68afd752"
      },
      "source": [
        "## Handle final response\n",
        "\n",
        "### Subtask:\n",
        "Process the model's final response after tool execution to extract the answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c7e5b3"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the final answer string from the `content` attribute of the `response_message` object after the loop concludes, and store it in `final_answer_content`. Print this variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdccbc1"
      },
      "source": [
        "# Assume the chat loop from the previous step has just completed.\n",
        "# The `response_message` variable holds the last message received from the model.\n",
        "# The `final_response_content` variable was intended to store the final answer.\n",
        "# The `messages` list holds the entire conversation history.\n",
        "\n",
        "# Check if the loop terminated because a final answer was received (i.e., no tool calls in the last message)\n",
        "# and if the last message is not empty.\n",
        "if response_message and not response_message.tool_calls and response_message.content:\n",
        "    final_answer_content = response_message.content\n",
        "    print(\"\\n--- Final Answer (Extracted after loop) ---\")\n",
        "    print(final_answer_content)\n",
        "elif turn_count >= max_turns:\n",
        "    print(\"\\n--- Chat loop ended due to reaching max turns ---\")\n",
        "    # If max turns reached, the last message might contain a partial answer or just model thoughts.\n",
        "    # We can still try to extract content if available, but it might not be a complete final answer.\n",
        "    if response_message and response_message.content:\n",
        "         final_answer_content = response_message.content\n",
        "         print(\"Last message content:\")\n",
        "         print(final_answer_content)\n",
        "    else:\n",
        "         final_answer_content = \"Chat loop ended without a final answer.\"\n",
        "         print(final_answer_content)\n",
        "else:\n",
        "    # Handle other potential loop exit conditions or errors\n",
        "    final_answer_content = \"Chat loop terminated unexpectedly without a final answer.\"\n",
        "    print(final_answer_content)\n",
        "\n",
        "# The final_answer_content variable now holds the extracted final answer or an informative message.\n",
        "# It can be used for further processing or evaluation.\n",
        "\n",
        "# Note: The previous cell's code already included printing logic after the loop.\n",
        "# This cell explicitly focuses on ensuring the extraction and storage in `final_answer_content`.\n",
        "# The printing is included here to demonstrate the result of the extraction."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dca1202"
      },
      "source": [
        "## Update agent usage\n",
        "\n",
        "### Subtask:\n",
        "Replace the `agent.chat()` calls with calls to your new Chat Completions based function/class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7ea363"
      },
      "source": [
        "**Reasoning**:\n",
        "Replace the existing agent.chat() calls in the training loop with calls to the new chat_with_tools function and integrate the logic to collect the final answer and the contexts retrieved during the tool execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2108398c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `Dataset` was not defined. This indicates that the import `from torch.utils.data import Dataset, DataLoader` was either not executed correctly or was not included in the necessary cell dependencies. I will add the import for `Dataset` and `DataLoader` again to ensure they are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1549f4c2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `policy_group` was not defined. This indicates that the code block responsible for creating the list of policy networks and their optimizers was not executed or was not included in the necessary dependencies. I will add the code to define `NUM_POLICIES`, instantiate the `RAGPolicyNetwork` instances, create the `policy_group` list, and create the corresponding `optimizers` list to ensure they are available before the training loop begins.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c62bee04"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `LEARNING_RATE` was not defined when trying to instantiate the optimizers. I need to ensure `LEARNING_RATE` is defined before the line where optimizers are created. I will add the definition for `LEARNING_RATE` and ensure all necessary imports and variable definitions from previous successful steps are present in this code block to make it runnable and self-contained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd9dbd04"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `optimizers` was not defined in the policy update phase. This is happening because the code block that defines and instantiates `policy_group` and `optimizers` is guarded by an `if 'policy_group' not in globals()` check. If the cell was run before and `policy_group` exists, this block is skipped, and `optimizers` is not defined in the current execution scope, even if it was defined in a *previous* execution of the cell. To fix this, I need to ensure `optimizers` is always defined when the training loop starts, regardless of whether the policy group was just instantiated or already existed. I will move the optimizer instantiation outside the `if/else` block for policy group instantiation, but still within the section that handles policy network setup, ensuring `LEARNING_RATE` and `policy_group` are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96e635ce"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `LEARNING_RATE` was not defined when creating the optimizers. This is happening because the definition of `LEARNING_RATE` was inside an `if` block that might have been skipped. I need to ensure `LEARNING_RATE` is defined unconditionally before it is used to create the optimizers. I will move the definition of `LEARNING_RATE` to the top of the cell, along with other essential variable definitions, ensuring it is always available. I will also include all necessary imports and variable definitions from previous successful steps to make this code block runnable and self-contained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1f8c8d9"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import openai\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Normal\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import wandb\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, SummaryIndex, StorageContext, load_index_from_storage\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "# Redefine necessary variables and functions from previous cells to ensure scope\n",
        "\n",
        "# Assuming OPENAI_API_KEY is already set as an environment variable\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    # Attempt to get from userdata if not in environment variables\n",
        "    api_key = userdata.get('OPENAI_API_KEY', default=None)\n",
        "    if api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "        print(\"Set OPENAI_API_KEY from Colab userdata.\")\n",
        "    else:\n",
        "        print(\"Error: OPENAI_API_KEY environment variable and Colab secret not set.\")\n",
        "\n",
        "\n",
        "# Setup OpenAI Model and Embeddings - Ensure these are set within this cell's execution\n",
        "# Check if Settings is already configured to avoid redundant calls if cell is re-run\n",
        "if not hasattr(Settings, '_llm') or Settings.llm is None:\n",
        "    Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "if not hasattr(Settings, '_embed_model') or Settings.embed_model is None:\n",
        "    Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "if not hasattr(Settings, '_chunk_size') or Settings.chunk_size != 1024:\n",
        "    Settings.chunk_size = 1024\n",
        "print(\"LlamaIndex Settings configured.\")\n",
        "\n",
        "\n",
        "# Assuming Google Drive is mounted at /content/drive and data_dir is defined\n",
        "if 'data_dir' not in globals():\n",
        "     data_dir = '/content/drive/MyDrive' # Define if not already\n",
        "if 'PERSIST_INDEX_DIR' not in globals():\n",
        "     PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\" # Define if not already\n",
        "\n",
        "\n",
        "# Redefine get_index function to ensure it's available and handles creation\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  full_index_dir = f\"{PERSIST_INDEX_DIR}{index_name}/\"\n",
        "  if not os.path.exists(full_index_dir):\n",
        "    print(f\"Index not found at {full_index_dir}. Attempting to create index...\")\n",
        "    # Load the documents\n",
        "    try:\n",
        "        documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "        print(f\"Loaded documents from {doc_file_path}.\")\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        print(\"Created VectorStoreIndex.\")\n",
        "        # Store the index to disk\n",
        "        os.makedirs(full_index_dir, exist_ok=True) # Ensure directory exists\n",
        "        index.storage_context.persist(full_index_dir)\n",
        "        print(f\"Created and persisted index at {full_index_dir}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Document file not found at {doc_file_path}. Cannot create index.\")\n",
        "        return None # Return None if document is not found\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during index creation: {e}\")\n",
        "        return None\n",
        "  else: # Load index from disk\n",
        "    print(f\"Loading index from storage at {full_index_dir}\")\n",
        "    try:\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=full_index_dir)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"Loaded index from storage.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during index loading from {full_index_dir}: {e}\")\n",
        "        return None\n",
        "\n",
        "  return index\n",
        "\n",
        "# Load or create the OECD index using the redefined get_index function\n",
        "# Ensure the path to the document is correct and the file exists\n",
        "oecd_doc_path = f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\"\n",
        "OECD_index = get_index(\"OECDTPGuidelines\", oecd_doc_path)\n",
        "\n",
        "# Assuming form990_guidelines_index is also needed and defined from previous cells\n",
        "# Add similar loading/creation logic for form990_guidelines_index if it's used by the tools\n",
        "form990_doc_path = f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\"\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\", form990_doc_path)\n",
        "\n",
        "\n",
        "# Redefine cosine_similarity_reward function if not available\n",
        "if 'cosine_similarity_reward' not in globals():\n",
        "    def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "        \"\"\"\n",
        "        Calculates a reward based on cosine similarity between the retrieved context\n",
        "        and the ground truth using TF-IDF vectorization.\n",
        "        \"\"\"\n",
        "        if not retrieved_context or not ground_truth:\n",
        "            return 0.0\n",
        "\n",
        "        # Handle case where one string is empty but the other isn't\n",
        "        if not retrieved_context or not ground_truth:\n",
        "             return 0.0 # Or some minimal penalty like 0.1 if one is empty\n",
        "\n",
        "        # Create TF-IDF vectors\n",
        "        vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "        vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        # Handle potential division by zero if vectors are zero vectors (e.g., empty strings after tokenization)\n",
        "        if vectors[0].sum() == 0 or vectors[1].sum() == 0:\n",
        "            return 0.0\n",
        "\n",
        "        similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "        return similarity_score\n",
        "\n",
        "# Redefine sample_action_and_continuous function if not available\n",
        "if 'sample_action_and_continuous' not in globals():\n",
        "    def sample_action_and_continuous(mean, log_variance):\n",
        "        std_dev = torch.exp(0.5 * log_variance)\n",
        "        distribution = Normal(mean, std_dev)\n",
        "        continuous_sample = distribution.sample()\n",
        "        # Ensure action is a positive integer\n",
        "        processed_action = torch.max(torch.tensor(1.0), torch.round(torch.abs(continuous_sample)))\n",
        "        return processed_action, continuous_sample\n",
        "\n",
        "# Redefine calculate_baseline function if not available\n",
        "if 'calculate_baseline' not in globals():\n",
        "    def calculate_baseline(rewards):\n",
        "        if isinstance(rewards, list):\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        if rewards.numel() == 0:\n",
        "            return 0.0\n",
        "        return torch.mean(rewards)\n",
        "\n",
        "# Redefine calculate_log_prob function if not available\n",
        "if 'calculate_log_prob' not in globals():\n",
        "    def calculate_log_prob(mean, log_variance, action):\n",
        "        std_dev = torch.exp(0.5 * log_variance)\n",
        "        distribution = Normal(mean, std_dev)\n",
        "        log_prob = distribution.log_prob(action)\n",
        "        return log_prob\n",
        "\n",
        "# Redefine RAGPolicyNetwork class if not available\n",
        "if 'RAGPolicyNetwork' not in globals():\n",
        "    class RAGPolicyNetwork(nn.Module):\n",
        "        def __init__(self, transformer_model_name=\"bert-base-uncased\", output_dim=2):\n",
        "            super(RAGPolicyNetwork, self).__init__()\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "            self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
        "            transformer_output_dim = self.transformer.config.hidden_size\n",
        "            self.output_layer = nn.Linear(transformer_output_dim, output_dim)\n",
        "\n",
        "        def forward(self, questions):\n",
        "            encoded_input = self.tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
        "            outputs = self.transformer(**encoded_input)\n",
        "            pooled_output = outputs.pooler_output\n",
        "            mean_and_log_variance = self.output_layer(pooled_output)\n",
        "            mean = mean_and_log_variance[:, 0]\n",
        "            log_variance = mean_and_log_variance[:, 1]\n",
        "            return mean, log_variance\n",
        "\n",
        "# Redefine questions and ground truth if not available\n",
        "if 'questions' not in globals() or 'ground_truth' not in globals():\n",
        "    questions = [\"What does Articles 9 of the OECD Model Tax Convention state?\",\n",
        "                 \"What does Articles 25 of the OECD Model Tax Convention state?\",\n",
        "                 \"What does Allocation of Taxing Rights mean in OECD Model Tax Convention state?\",\n",
        "                 \"How is Mutual Agreement Procedure(MAP) help in resolving disputes between countries when there's a conflict in interpreting the treaty?\",\n",
        "                 \"As per OECD Model Tax Convention States what does Residence and Source Country mean?\"]\n",
        "    ground_truth = [\"addresses corresponding adjustments in transfer pricing\",\n",
        "                    \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\",\n",
        "                    \"principles that determine how different jurisdictions can tax income generated by multinational enterprises (MNEs).\",\n",
        "                    \"serves as a mechanism for tax administrations to consult and resolve disputes related to the interpretation and application of double tax conventions. It is particularly useful in situations where there is taxation not in accordance with the provisions of the Convention.\",\n",
        "                    \"Resident country: The country where the taxpayer lives, Source country: The country where the income originates may also have taxing rights but often with limits.\"]\n",
        "\n",
        "\n",
        "# Redefine Dataset and DataLoader if not available\n",
        "if 'RAGDataset' not in globals() or 'train_dataloader' not in globals():\n",
        "    class RAGDataset(Dataset):\n",
        "        def __init__(self, questions, ground_truth):\n",
        "            self.questions = questions\n",
        "            self.ground_truth = ground_truth\n",
        "        def __len__(self):\n",
        "            return len(self.questions)\n",
        "        def __getitem__(self, idx):\n",
        "            return self.questions[idx], self.ground_truth[idx]\n",
        "\n",
        "    rag_dataset = RAGDataset(questions, ground_truth)\n",
        "    BATCH_SIZE = 8 # Define BATCH_SIZE if not already\n",
        "    train_dataloader = DataLoader(rag_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    NUM_EPOCHS = 100 # Define NUM_EPOCHS if not already\n",
        "    LEARNING_RATE = 1e-4 # Define LEARNING_RATE if not already\n",
        "\n",
        "\n",
        "# Re-instantiate policy_group and optimizers if not available (important for fresh run)\n",
        "if 'policy_group' not in globals():\n",
        "    NUM_POLICIES = 5 # Define NUM_POLICIES if not already\n",
        "    policy_group = nn.ModuleList()\n",
        "    for i in range(NUM_POLICIES):\n",
        "        policy = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "        policy_group.append(policy)\n",
        "    print(f\"Re-instantiated a group of {NUM_POLICIES} RAGPolicyNetwork instances.\")\n",
        "elif len(policy_group) != NUM_POLICIES:\n",
        "     print(f\"Policy group size mismatch. Re-instantiating {NUM_POLICIES} policies.\")\n",
        "     policy_group = nn.ModuleList()\n",
        "     for i in range(NUM_POLICIES):\n",
        "        policy = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "        policy_group.append(policy)\n",
        "     print(f\"Re-instantiated a group of {NUM_POLICIES} RAGPolicyNetwork instances.\")\n",
        "else:\n",
        "    print(f\"Policy group with {NUM_POLICIES} instances already exists.\")\n",
        "\n",
        "# Ensure optimizers are defined for each policy\n",
        "if 'optimizers' not in globals() or len(optimizers) != NUM_POLICIES:\n",
        "     print(f\"Optimizers not defined or count mismatch. Creating {NUM_POLICIES} optimizers.\")\n",
        "     optimizers = [optim.Adam(policy.parameters(), lr=LEARNING_RATE) for policy in policy_group]\n",
        "else:\n",
        "    print(f\"Optimizers for {NUM_POLICIES} policies already exists.\")\n",
        "\n",
        "\n",
        "# Redefine tools (LlamaIndex QueryEngineTool instances) and openai_tools (OpenAI definitions)\n",
        "# Assuming these were defined in previous cells and need to be available here.\n",
        "# If not defined, create dummy tools or add logic to load/create them.\n",
        "if 'tools' not in globals() or not tools:\n",
        "    print(\"LlamaIndex tools not found. Creating tools from loaded indices.\")\n",
        "    tools = []\n",
        "    if OECD_index is not None:\n",
        "        oecd_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "        oecd_tool = QueryEngineTool(query_engine=oecd_engine, metadata=ToolMetadata(name=\"OECD_QueryEngineTool_2022\", description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"))\n",
        "        tools.append(oecd_tool)\n",
        "    if form990_guidelines_index is not None:\n",
        "        form990_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "        form990_tool = QueryEngineTool(query_engine=form990_engine, metadata=ToolMetadata(name=\"form990_2022\", description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"))\n",
        "        tools.append(form990_tool)\n",
        "\n",
        "    if tools:\n",
        "        print(f\"Created {len(tools)} LlamaIndex tools.\")\n",
        "        # Create openai_tools from LlamaIndex tools\n",
        "        openai_tools = [openai_tool_definition(tool) for tool in tools]\n",
        "        print(\"Created OpenAI tool definitions.\")\n",
        "    else:\n",
        "        print(\"Could not create any LlamaIndex tools. Tool execution will likely fail.\")\n",
        "        openai_tools = [] # Ensure openai_tools is an empty list if no tools were created\n",
        "\n",
        "elif 'openai_tools' not in globals() or not openai_tools:\n",
        "    print(\"OpenAI tool definitions not found. Creating from existing LlamaIndex tools.\")\n",
        "    openai_tools = [openai_tool_definition(tool) for tool in tools]\n",
        "    print(\"Created OpenAI tool definitions from existing LlamaIndex tools.\")\n",
        "else:\n",
        "    print(\"LlamaIndex tools and OpenAI tool definitions already exist.\")\n",
        "\n",
        "\n",
        "# Redefine query_engine_map if not available or needs update\n",
        "if 'query_engine_map' not in globals() or not query_engine_map:\n",
        "    print(\"Query engine map not found or is empty. Creating map.\")\n",
        "    query_engine_map = {}\n",
        "    if 'tools' in globals() and tools:\n",
        "        for tool in tools:\n",
        "            # Assuming the tool's query_engine is accessible\n",
        "            query_engine_map[tool.metadata.name] = tool.query_engine\n",
        "        print(f\"Created query engine map with {len(query_engine_map)} entries.\")\n",
        "    else:\n",
        "        print(\"LlamaIndex tools not available to create query engine map.\")\n",
        "else:\n",
        "    print(\"Query engine map already exists.\")\n",
        "\n",
        "\n",
        "# Initialize a Weights & Biases run (if not already initialized and active)\n",
        "# Use reinit=True to allow re-initialization in a notebook environment\n",
        "if wandb.run is None: # Corrected check for initialization\n",
        "    try:\n",
        "        # Ensure wandb is imported and initialized before logging\n",
        "        if 'wandb' not in globals():\n",
        "             import wandb\n",
        "        wandb.init(project=\"rag-policy-training\", name=\"grpo-cosine-similarity-group-update-chat-api\", reinit=True)\n",
        "\n",
        "        # Define and log hyperparameters\n",
        "        config = {\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"num_epochs\": NUM_EPOCHS,\n",
        "            \"transformer_model\": \"bert-base-uncased\",\n",
        "            \"output_dim\": 2,\n",
        "            \"num_policies\": NUM_POLICIES,\n",
        "            \"max_chat_turns\": 5 # Log max chat turns\n",
        "        }\n",
        "        wandb.config.update(config)\n",
        "        print(\"Training hyperparameters logged to Weights & Biases config.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Weights & Biases: {e}\")\n",
        "        print(\"Weights & Biases logging will be skipped.\")\n",
        "elif wandb.run is not None:\n",
        "    print(f\"Weights & Biases run '{wandb.run.name}' is already active.\")\n",
        "    # Optionally update config if needed, though reinit=True handles this to some extent\n",
        "    # wandb.config.update(config, allow_val_change=True)\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"Starting policy group training with Chat Completions API...\")\n",
        "\n",
        "# Calculate total steps for logging (already done, but ensure variable exists)\n",
        "if 'total_steps' not in globals() and 'NUM_EPOCHS' in globals() and 'train_dataloader' in globals() and 'NUM_POLICIES' in globals():\n",
        "     total_steps = NUM_EPOCHS * len(train_dataloader) * NUM_POLICIES\n",
        "if 'global_step' not in globals():\n",
        "     global_step = 0\n",
        "if 'NUM_EPOCHS' not in globals():\n",
        "     NUM_EPOCHS = 100 # Define if not already\n",
        "if 'MAX_GRAD_NORM' not in globals():\n",
        "     MAX_GRAD_NORM = 0.5 # Define if not already\n",
        "\n",
        "\n",
        "# Define max turns for the chat conversation within the training loop\n",
        "max_turns_chat = 5 # Limit the number of turns for each question's chat interaction\n",
        "\n",
        "\n",
        "# Check if OECD_index was loaded successfully before starting training\n",
        "if OECD_index is not None and form990_guidelines_index is not None: # Ensure both indices are loaded if both tools are intended\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Data structures to collect data across policies for this iteration/epoch\n",
        "        all_policy_rewards = {}\n",
        "        all_policy_log_probs = {}\n",
        "        all_policy_sampled_k_processed = {}\n",
        "        all_policy_advantages = {}\n",
        "        all_policy_means = {}\n",
        "        all_policy_log_variances = {}\n",
        "        all_policy_losses = {} # Store losses for logging per policy\n",
        "        collected_answers = {} # Store generated answers\n",
        "        collected_contexts = {} # Store retrieved contexts\n",
        "\n",
        "        # --- Data Collection Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Collecting data using Chat Completions API...\")\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy.train() # Set policy to training mode\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "\n",
        "            # Initialize storage for current policy's data\n",
        "            all_policy_rewards[policy_name] = []\n",
        "            all_policy_log_probs[policy_name] = []\n",
        "            all_policy_sampled_k_processed[policy_name] = []\n",
        "            all_policy_means[policy_name] = []\n",
        "            all_policy_log_variances[policy_name] = []\n",
        "            all_policy_losses[policy_name] = [] # Initialize loss storage\n",
        "            collected_answers[policy_name] = []\n",
        "            collected_contexts[policy_name] = []\n",
        "\n",
        "\n",
        "            # Process the entire dataset for the current policy to collect data\n",
        "            for batch_idx, (batch_questions, batch_ground_truth) in enumerate(train_dataloader):\n",
        "                if not batch_questions:\n",
        "                    continue # Skip empty batches\n",
        "\n",
        "                # global_step += 1 # Decide if global step increments per batch or per policy pass over data\n",
        "                                 # Let's increment per batch processed by any policy for overall progress tracking later\n",
        "\n",
        "                # Perform a forward pass through the policy network for the batch\n",
        "                mean_output, log_variance_output = policy(list(batch_questions))\n",
        "\n",
        "                for i in range(len(batch_questions)):\n",
        "                    question = batch_questions[i]\n",
        "                    ground_truth_answer = batch_ground_truth[i]\n",
        "\n",
        "                    # b. Use the sample_action_and_continuous function to sample similarity_top_k actions\n",
        "                    sampled_k_processed_item, sampled_k_continuous_item = sample_action_and_continuous(mean_output[i], log_variance_output[i])\n",
        "\n",
        "                    # Store sampled action and log probability for this sample\n",
        "                    all_policy_sampled_k_processed[policy_name].append(sampled_k_processed_item.item())\n",
        "                    all_policy_log_probs[policy_name].append(calculate_log_prob(mean_output[i], log_variance_output[i], sampled_k_continuous_item).item())\n",
        "                    all_policy_means[policy_name].append(mean_output[i].item())\n",
        "                    all_policy_log_variances[policy_name].append(log_variance_output[i].item())\n",
        "\n",
        "                    # --- Integrate Actual RAG Execution using Chat Completions API ---\n",
        "                    predicted_top_k_int = max(1, int(sampled_k_processed_item.item())) # Ensure it's at least 1\n",
        "\n",
        "                    # Initialize messages for the chat conversation for this specific question\n",
        "                    messages = [{\"role\": \"system\", \"content\": \"You are an assistant that provides answers to questions on OECD and Form990 using the available tools. Answer as accurately as possible based on the tool outputs. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\"}]\n",
        "                    messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "                    chat_turn_count = 0\n",
        "                    generated_answer = \"\"\n",
        "                    retrieved_contexts_for_sample = [] # Store contexts for this sample\n",
        "\n",
        "                    while chat_turn_count < max_turns_chat:\n",
        "                        chat_turn_count += 1\n",
        "                        # print(f\"    Chat Turn {chat_turn_count}\") # Too verbose\n",
        "\n",
        "                        # 2. Send messages to the model using chat_with_tools\n",
        "                        api_response = chat_with_tools(messages, openai_tools)\n",
        "\n",
        "                        if not api_response or not hasattr(api_response, 'choices') or not api_response.choices:\n",
        "                            print(f\"    API call failed or returned no choices for question: {question}\")\n",
        "                            break\n",
        "\n",
        "                        response_message = api_response.choices[0].message\n",
        "                        messages.append(response_message) # Append model's response to history\n",
        "\n",
        "                        tool_calls = response_message.tool_calls\n",
        "\n",
        "                        if tool_calls:\n",
        "                            # 4. Iterate through each tool call\n",
        "                            for tool_call in tool_calls:\n",
        "                                function_name = tool_call.function.name\n",
        "                                function_args_str = tool_call.function.arguments\n",
        "                                tool_call_id = tool_call.id\n",
        "\n",
        "                                # print(f\"      Tool call requested: {function_name}\") # Too verbose\n",
        "\n",
        "                                # 5. Parse the arguments string\n",
        "                                try:\n",
        "                                    function_args = json.loads(function_args_str)\n",
        "                                    tool_input = function_args.get(\"input\")\n",
        "                                    if tool_input is None:\n",
        "                                         print(f\"      Warning: 'input' arg missing for {function_name}.\")\n",
        "                                         tool_input = \"\"\n",
        "\n",
        "                                except json.JSONDecodeError:\n",
        "                                    print(f\"      Error decoding args JSON for {function_name}: {function_args_str}\")\n",
        "                                    tool_input = \"\"\n",
        "\n",
        "                                # 6. Identify and execute the corresponding LlamaIndex query engine\n",
        "                                # Use the policy-predicted similarity_top_k for the query engine\n",
        "                                query_engine = query_engine_map.get(function_name)\n",
        "                                tool_output = \"\"\n",
        "\n",
        "                                if query_engine:\n",
        "                                    # Temporarily set similarity_top_k for this query engine if it supports it\n",
        "                                    original_similarity_top_k = getattr(query_engine.retriever, 'similarity_top_k', None)\n",
        "                                    if original_similarity_top_k is not None:\n",
        "                                         query_engine.retriever.similarity_top_k = predicted_top_k_int\n",
        "                                         # print(f\"      Set similarity_top_k to {predicted_top_k_int} for {function_name}\") # Too verbose\n",
        "\n",
        "                                    try:\n",
        "                                        llama_response = query_engine.query(tool_input)\n",
        "                                        tool_output = str(llama_response)\n",
        "                                        # Collect the retrieved nodes' text as context\n",
        "                                        if hasattr(llama_response, 'source_nodes'):\n",
        "                                             retrieved_contexts_for_sample.extend([node.node.text for node in llama_response.source_nodes])\n",
        "\n",
        "                                        # print(f\"      Tool execution successful.\") # Too verbose\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"      Error executing LlamaIndex tool '{function_name}': {e}\")\n",
        "                                        tool_output = f\"Error executing tool: {e}\"\n",
        "\n",
        "                                    finally:\n",
        "                                         # Restore original similarity_top_k if it was modified\n",
        "                                         if original_similarity_top_k is not None:\n",
        "                                              query_engine.retriever.similarity_top_k = original_similarity_top_k\n",
        "\n",
        "\n",
        "                                else:\n",
        "                                    print(f\"      Error: No LlamaIndex query engine found for tool '{function_name}'.\")\n",
        "                                    tool_output = f\"Error: Tool '{function_name}' not supported or found.\"\n",
        "\n",
        "                                # 7. Format the output and 8. Append to messages history\n",
        "                                tool_message = {\n",
        "                                    \"role\": \"tool\",\n",
        "                                    \"tool_call_id\": tool_call_id,\n",
        "                                    \"content\": tool_output,\n",
        "                                }\n",
        "                                messages.append(tool_message)\n",
        "                                # print(\"      Tool output appended.\") # Too verbose\n",
        "\n",
        "                            # 9. The loop continues, sending updated messages in the next turn\n",
        "                        else:\n",
        "                            # 10. If no tool_calls, it's the final answer\n",
        "                            # print(\"    Model provided final answer.\") # Too verbose\n",
        "                            generated_answer = response_message.content\n",
        "                            break # Exit chat loop for this question\n",
        "\n",
        "                    # --- End of Chat Conversation for one question ---\n",
        "\n",
        "                    # Store generated answer and collected contexts for this sample\n",
        "                    collected_answers[policy_name].append(generated_answer)\n",
        "                    collected_contexts[policy_name].append(retrieved_contexts_for_sample)\n",
        "\n",
        "\n",
        "                    # Calculate the cosine similarity reward for the generated answer\n",
        "                    # Use the combined retrieved contexts for reward calculation baseline if needed,\n",
        "                    # but the reward is typically based on the *generated answer* vs *ground truth*.\n",
        "                    # The collected contexts are for evaluation/analysis, not directly for the reward calculation here.\n",
        "                    # The reward function already compares generated_answer and ground_truth_answer.\n",
        "                    reward = cosine_similarity_reward(generated_answer, ground_truth_answer)\n",
        "                    all_policy_rewards[policy_name].append(reward)\n",
        "\n",
        "                # Increment global step once per batch processed by any policy\n",
        "                global_step += 1\n",
        "\n",
        "\n",
        "        # --- End of Policy Data Collection within Epoch ---\n",
        "\n",
        "        # --- Implement Group Performance Evaluation and Logging ---\n",
        "        policy_avg_rewards = {}\n",
        "        best_policy_name = None\n",
        "        highest_avg_reward = -float('inf') # Initialize with negative infinity\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Evaluating policy performance and logging epoch metrics...\")\n",
        "\n",
        "        # Data structure to store epoch metrics for logging\n",
        "        epoch_metrics = {}\n",
        "        group_avg_reward = 0.0\n",
        "        total_valid_rewards = 0\n",
        "\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            epoch_rewards = all_policy_rewards[policy_name]\n",
        "\n",
        "            # 1. Calculate the average reward for each policy\n",
        "            avg_epoch_reward = np.mean(epoch_rewards) if epoch_rewards else 0.0\n",
        "            policy_avg_rewards[policy_name] = avg_epoch_reward\n",
        "\n",
        "            # 3. Identify the policy with the highest average reward\n",
        "            if avg_epoch_reward > highest_avg_reward:\n",
        "                highest_avg_reward = avg_epoch_reward\n",
        "                best_policy_name = policy_name\n",
        "\n",
        "            # Also calculate other epoch metrics for logging\n",
        "            epoch_predicted_k = all_policy_sampled_k_processed[policy_name]\n",
        "            # Recalculate advantages based on the actual epoch rewards collected\n",
        "            epoch_rewards_tensor = torch.tensor(epoch_rewards, dtype=torch.float32)\n",
        "            epoch_advantages = (epoch_rewards_tensor - calculate_baseline(epoch_rewards_tensor)).tolist() # Use individual policy's baseline for advantage calculation\n",
        "\n",
        "            epoch_means = all_policy_means[policy_name]\n",
        "            epoch_log_variances = all_policy_log_variances[policy_name]\n",
        "\n",
        "            avg_epoch_predicted_top_k = np.mean(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            epoch_predicted_top_k_std = np.std(epoch_predicted_k) if epoch_predicted_k else 0\n",
        "            avg_epoch_advantage = np.mean(epoch_advantages) if epoch_advantages else 0\n",
        "            avg_epoch_mean = np.mean(epoch_means) if epoch_means else 0\n",
        "            avg_epoch_log_variance = np.mean(epoch_log_variances) if epoch_log_variances else 0\n",
        "\n",
        "            # Store policy-specific epoch metrics for logging\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_reward\"] = avg_epoch_reward\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_predicted_top_k\"] = avg_epoch_predicted_top_k\n",
        "            epoch_metrics[f\"{policy_name}/epoch_predicted_top_k_std\"] = epoch_predicted_top_k_std\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_advantage\"] = avg_epoch_advantage\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_mean\"] = avg_epoch_mean\n",
        "            epoch_metrics[f\"{policy_name}/epoch_average_log_variance\"] = avg_epoch_log_variance\n",
        "\n",
        "\n",
        "            # Accumulate reward for group average calculation\n",
        "            group_avg_reward += np.sum(epoch_rewards)\n",
        "            total_valid_rewards += len(epoch_rewards) # Sum of samples across all policies\n",
        "\n",
        "\n",
        "            # 4. Print or store the average rewards for each policy\n",
        "            print(f\"    {policy_name}: Avg Reward = {avg_epoch_reward:.4f}, Avg Predicted Top K = {avg_epoch_predicted_top_k:.2f}, Predicted Top K Std = {epoch_predicted_top_k_std:.2f}\")\n",
        "\n",
        "\n",
        "        # Calculate group-level average reward across all policies\n",
        "        group_avg_reward = group_avg_reward / total_valid_rewards if total_valid_rewards > 0 else 0.0\n",
        "\n",
        "\n",
        "        # 4. Print or store the identification of the best performing policy\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Best performing policy is {best_policy_name} with Avg Reward = {highest_avg_reward:.4f}\")\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Group Average Reward = {group_avg_reward:.4f}\")\n",
        "\n",
        "\n",
        "        # Log epoch metrics to Weights & Biases\n",
        "        epoch_metrics[\"epoch/best_policy\"] = best_policy_name\n",
        "        epoch_metrics[\"epoch/highest_avg_reward\"] = highest_avg_reward\n",
        "        epoch_metrics[\"epoch/group_average_reward\"] = group_avg_reward # Log group average reward\n",
        "\n",
        "        # Log all collected epoch metrics\n",
        "        if wandb.run is not None:\n",
        "             wandb.log(epoch_metrics, step=epoch + 1) # Log all epoch metrics at once\n",
        "        else:\n",
        "             print(\"Weights & Biases not initialized. Skipping epoch metric logging.\")\n",
        "\n",
        "\n",
        "        # --- Implement Policy Update Phase ---\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Starting policy update...\")\n",
        "\n",
        "        # --- Subtask Implementation: Modify policy update step ---\n",
        "        # Use the average reward of the best policy as a baseline for all policies\n",
        "        # Or use the individual policy's advantage relative to the best policy's average reward.\n",
        "        # Let's use the individual policy's advantage calculated against its own baseline for simplicity in this step,\n",
        "        # but consider the relative performance implicitly by comparing policies.\n",
        "        # A more GRPO-like approach might use the best policy's average reward as a global baseline.\n",
        "        # Let's modify the advantage calculation to use the best policy's average reward as the baseline.\n",
        "\n",
        "        best_policy_avg_reward_tensor = torch.tensor(highest_avg_reward, dtype=torch.float32) # Get the best policy's avg reward as a tensor\n",
        "\n",
        "\n",
        "        for policy_idx, policy in enumerate(policy_group):\n",
        "            policy_name = f\"policy_{policy_idx}\"\n",
        "            optimizer = optimizers[policy_idx] # Get the specific optimizer for this policy\n",
        "\n",
        "            # Get collected data for the current policy\n",
        "            policy_log_probs = torch.tensor(all_policy_log_probs[policy_name], dtype=torch.float32)\n",
        "            policy_rewards = torch.tensor(all_policy_rewards[policy_name], dtype=torch.float32) # Use raw rewards\n",
        "\n",
        "            # Calculate advantage relative to the best policy's average reward\n",
        "            # Advantage = Reward - Best Policy's Average Reward\n",
        "            policy_advantages_relative_to_best = policy_rewards - best_policy_avg_reward_tensor\n",
        "\n",
        "            # Filter out samples where original reward was 0 (likely due to errors)\n",
        "            # We still use the original rewards to determine which samples were valid.\n",
        "            valid_indices = policy_rewards != 0\n",
        "            if torch.sum(valid_indices) > 0:\n",
        "                valid_log_probs = policy_log_probs[valid_indices]\n",
        "                # Detach advantages to prevent gradients from flowing through the reward calculation\n",
        "                valid_advantages = policy_advantages_relative_to_best[valid_indices].detach()\n",
        "\n",
        "\n",
        "                # 3. Calculate the policy loss using collected log probabilities and relative advantages\n",
        "                policy_loss = -torch.mean(valid_log_probs * valid_advantages)\n",
        "\n",
        "                # 5. Perform optimizer.zero_grad() for the current policy's optimizer\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # --- Implement basic \"trust region\" with gradient clipping ---\n",
        "                # 6. Call policy_loss.backward() to compute gradients\n",
        "                policy_loss.backward()\n",
        "\n",
        "                # Apply gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "\n",
        "                # 7. Call optimizer.step() to update the current policy's parameters\n",
        "                optimizer.step()\n",
        "\n",
        "                # Log the policy loss for each policy after its update\n",
        "                if wandb.run is not None:\n",
        "                     wandb.log({\n",
        "                         f\"{policy_name}/policy_loss\": policy_loss.item(),\n",
        "                     }, step=epoch + 1) # Log policy loss per epoch per policy\n",
        "                else:\n",
        "                     print(f\"Weights & Biases not initialized. Skipping policy loss logging for {policy_name}.\")\n",
        "\n",
        "\n",
        "                # print(f\"    {policy_name}: Policy loss = {policy_loss.item():.4f}\") # Too verbose\n",
        "            else:\n",
        "                # print(f\"    {policy_name}: No valid samples/advantages for update in this epoch.\") # Too verbose\n",
        "                if wandb.run is not None:\n",
        "                     wandb.log({\n",
        "                         f\"{policy_name}/policy_loss\": 0.0, # Log 0 loss if no update\n",
        "                     }, step=epoch + 1)\n",
        "                else:\n",
        "                     print(f\"Weights & Biases not initialized. Skipping policy loss logging for {policy_name} (no update).\")\n",
        "\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: Policy update completed.\")\n",
        "\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training skipped because OECD index was not loaded due to missing document.\")\n",
        "\n",
        "# Finish the Weights & Biases run\n",
        "if wandb.run is not None:\n",
        "    # Corrected check for finishing run\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}